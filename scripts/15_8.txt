
=== Parallel Configuration Summary ===
DP:                        1 
TP:                        4
SP:                        Enabled
PP:                        1
VP:                        None
CP:                        2
Activation Recomputation:  Disabled
Micro Batch Size:          8
Global Batch Size:         8
======================================

──────── Entering Experiment mistral_7b_pretraining with id: mistral_7b_pretraining_1761285448 ─────────
[05:57:28] INFO     Log directory is:                                             local_scheduler.py:777
                    /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b                       
                    _pretraining_1761285448/mistral_7b_pretraining                                      
[05:57:28] Launching job mistral_7b_pretraining for experiment mistral_7b_pretraining  experiment.py:771
           INFO     Log directory is:                                             local_scheduler.py:777
                    /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b                       
                    _pretraining_1761285448/mistral_7b_pretraining                                      
           INFO     Launched app:                                                        launcher.py:111
                    local_persistent://nemo_run/mistral_7b_pretraining-r131pr794wpj0c                   
────────────────── Waiting for Experiment mistral_7b_pretraining_1761285448 to finish ──────────────────

Experiment Status for mistral_7b_pretraining_1761285448

Task 0: mistral_7b_pretraining
- Status: RUNNING
- Executor: LocalExecutor
- Job id: mistral_7b_pretraining-r131pr794wpj0c
- Local Directory: /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b_pretraining_1761285448/mistral_7b_pretraining

           INFO     Waiting for job mistral_7b_pretraining-r131pr794wpj0c to finish      launcher.py:131
                    [log=True]...                                                                       
retraining/0 I1024 05:57:30.112000 106527 torch/distributed/run.py:649] Using nproc_per_node=8.
retraining/0 W1024 05:57:30.113000 106527 torch/distributed/run.py:766] 
retraining/0 W1024 05:57:30.113000 106527 torch/distributed/run.py:766] *****************************************
retraining/0 W1024 05:57:30.113000 106527 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
retraining/0 W1024 05:57:30.113000 106527 torch/distributed/run.py:766] *****************************************
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195] Starting elastic_operator with launch configs:
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   entrypoint       : nemo_run.core.runners.fdl_runner
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   min_nodes        : 1
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   max_nodes        : 1
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   nproc_per_node   : 8
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   run_id           : 3253
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   rdzv_backend     : c10d
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   rdzv_endpoint    : localhost:0
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   rdzv_configs     : {'timeout': 900}
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   max_restarts     : 0
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   monitor_interval : 0.1
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   log_dir          : /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b_pretraining_1761285448/mistral_7b_pretraining/nemo_run/mistral_7b_pretraining-r131pr794wpj0c/torchelastic/mistral_7b_pretraining
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195]   metrics_cfg      : {}
retraining/0 I1024 05:57:30.114000 106527 torch/distributed/launcher/api.py:195] 
retraining/0 I1024 05:57:30.117000 106527 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python
retraining/0 I1024 05:57:30.118000 106527 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   master_addr=cc76e080f006
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   master_port=42317
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:525] 
retraining/0 I1024 05:57:30.202000 106527 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group
retraining/0 I1024 05:57:30.203000 106527 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True
retraining/0 I1024 05:57:30.203000 106527 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
retraining/0 I1024 05:57:30.204000 106527 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.
retraining/0 [default0]:[NeMo W 2025-10-24 05:57:47 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
retraining/0 [default0]:      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
retraining/0 [default0]:    
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
retraining/0 [default6]:[W1024 05:57:49.917338401 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default7]:[W1024 05:57:49.963957347 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default3]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default3]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default3]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default7]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default7]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default7]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default3]:[W1024 05:57:50.129604473 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default/2025-10-24_05-57-50
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has data parallel group : [0]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 4]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Ranks 0 has data parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has context parallel group: [0, 4]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Ranks 0 has context parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has model parallel group: [0, 1, 2, 3]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has tensor model parallel group: [0, 1, 2, 3]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has embedding group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 0 has embedding rank: 0
retraining/0 [default0]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default0]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default0]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default4]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default4]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default4]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default0]:GPU available: True (cuda), used: True
retraining/0 [default0]:TPU available: False, using: 0 TPU cores
retraining/0 [default0]:HPU available: False, using: 0 HPUs
retraining/0 [default0]:[NeMo W 2025-10-24 05:57:50 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
retraining/0 [default0]:You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
retraining/0 [default0]:[W1024 05:57:50.568022082 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:----------------------------------------------------------------------------------------------------
retraining/0 [default0]:distributed_backend=nccl
retraining/0 [default0]:All distributed processes registered. Starting with 8 processes
retraining/0 [default0]:----------------------------------------------------------------------------------------------------
retraining/0 [default0]:
retraining/0 [default4]:[W1024 05:57:50.609453796 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default6]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default6]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default6]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default2]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default2]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default2]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default2]:[W1024 05:57:50.727843850 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default5]:[W1024 05:57:50.698341770 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default5]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default5]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default5]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default1]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default1]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default1]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default1]:[W1024 05:57:51.696617144 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] Let split_matrix = [(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] Building GPTDataset splits with sizes=[160, 0, 8] and config=GPTDatasetConfig(random_seed=1234, sequence_length=8192, blend=[['/root/dataset/openwebtext'], None], blend_per_split=None, split='900,50,50', split_matrix=[(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x7f762242d940>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] Load the _IndexReader from /root/dataset/openwebtext.idx
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Extract the sequence lengths
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Extract the sequence pointers
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Extract the document indices
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] > total number of sequences: 8013769
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] > total number of documents: 8013769
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] Load the GPTDataset train indices
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the document index from 75cec36a1003a70065068375d6a00bb6-GPTDataset-train-document_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the sample index from 75cec36a1003a70065068375d6a00bb6-GPTDataset-train-sample_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the shuffle index from 75cec36a1003a70065068375d6a00bb6-GPTDataset-train-shuffle_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] > total number of samples: 974208
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] Load the GPTDataset valid indices
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the document index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-document_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the sample index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-sample_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the shuffle index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-shuffle_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] > total number of samples: 53956
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] Load the GPTDataset test indices
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the document index from 841d3655d4e982e55bcc6fbfb9396d09-GPTDataset-test-document_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the sample index from 841d3655d4e982e55bcc6fbfb9396d09-GPTDataset-test-sample_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] 	Load the shuffle index from 841d3655d4e982e55bcc6fbfb9396d09-GPTDataset-test-shuffle_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 utils:661] > total number of samples: 54137
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:52 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
retraining/0 [default0]:[NeMo W 2025-10-24 05:57:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:53 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:53 num_microbatches_calculator:228] setting number of microbatches to constant 1
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1797001216
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:53 utils:661] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, reuse_grad_buf_for_mxfp8_param_ag=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False, nccl_ub=False, fsdp_double_buffer=False)
retraining/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default4]:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default6]:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default3]:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default2]:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default5]:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default7]:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:53 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
retraining/0 [default0]:    Params for bucket 1 (1797001216 elements, 1797001216 padded size):
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.embedding.word_embeddings.weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.final_layernorm.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc2.weight
retraining/0 [default0]:[NeMo I 2025-10-24 05:57:53 utils:661] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=9e-05, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp8_recipe='delayed', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
retraining/0 [default0]:
retraining/0 [default0]:  | Name   | Type | Params | Mode 
retraining/0 [default0]:----------------------------------------
retraining/0 [default0]:0 | module | DDP  | 1.8 B  | train
retraining/0 [default0]:----------------------------------------
retraining/0 [default0]:1.8 B     Trainable params
retraining/0 [default0]:0         Non-trainable params
retraining/0 [default0]:1.8 B     Total params
retraining/0 [default0]:7,188.005 Total estimated model params size (MB)
retraining/0 [default0]:651       Modules in train mode
retraining/0 [default0]:0         Modules in eval mode
retraining/0 [default0]:[NeMo W 2025-10-24 05:58:01 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
retraining/0 [default0]:[NeMo W 2025-10-24 05:58:01 rerun_state_machine:239] RerunStateMachine initialized in mode RerunMode.DISABLED
retraining/0 [default0]:Training epoch 0, iteration 9/19 | lr: 2.474e-07 | global_batch_size: 8 | global_step: 9 | reduced_train_loss: 11.66 | train_step_timing in s: 3.099 | tokens_per_sec_per_gpu: 3.089e+03 | consumed_samples: 43.2
retraining/0 [default6]:[rank6]: Traceback (most recent call last):
retraining/0 [default6]:[rank6]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default6]:[rank6]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default6]:[rank6]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default6]:[rank6]:     fdl_runner_app()
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default6]:[rank6]:     raise e
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default6]:[rank6]:     return get_command(self)(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default6]:[rank6]:     return self.main(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default6]:[rank6]:     return _main(
retraining/0 [default6]:[rank6]:            ^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default6]:[rank6]:     rv = self.invoke(ctx)
retraining/0 [default6]:[rank6]:          ^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default6]:[rank6]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default6]:[rank6]:     return __callback(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default6]:[rank6]:     return callback(**use_params)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default6]:[rank6]:     fdl_fn()
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default6]:[rank6]:     return train(
retraining/0 [default6]:[rank6]:            ^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default6]:[rank6]:     trainer.fit(model, data)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default6]:[rank6]:     call._call_and_handle_interrupt(
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default6]:[rank6]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default6]:[rank6]:     return function(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default6]:[rank6]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default6]:[rank6]:     results = self._run_stage()
retraining/0 [default6]:[rank6]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default6]:[rank6]:     self.fit_loop.run()
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default6]:[rank6]:     self.advance()
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default6]:[rank6]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default6]:[rank6]:     self.advance(data_fetcher)
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default6]:[rank6]:     super().advance(data_fetcher)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default6]:[rank6]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default6]:[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default6]:[rank6]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default6]:[rank6]:     call._call_lightning_module_hook(
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default6]:[rank6]:     output = fn(*args, **kwargs)
retraining/0 [default6]:[rank6]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default6]:[rank6]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default6]:[rank6]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default6]:[rank6]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default6]:[rank6]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default6]:[rank6]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default6]:[rank6]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default6]:[rank6]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default6]:[rank6]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default6]:[rank6]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default6]:[rank6]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default6]:[rank6]:     loss = closure()
retraining/0 [default6]:[rank6]:            ^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default6]:[rank6]:     closure_result = closure()
retraining/0 [default6]:[rank6]:                      ^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default6]:[rank6]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default6]:[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default6]:[rank6]:     return func(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default6]:[rank6]:     step_output = self._step_fn()
retraining/0 [default6]:[rank6]:                   ^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default6]:[rank6]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default6]:[rank6]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default6]:[rank6]:     output = fn(*args, **kwargs)
retraining/0 [default6]:[rank6]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default6]:[rank6]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default6]:[rank6]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default6]:[rank6]:     return self._step(
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default6]:[rank6]:     return self.forward(
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default6]:[rank6]:     microbatch_outputs = step()
retraining/0 [default6]:[rank6]:                          ^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default6]:[rank6]:     return self.forward_backward_func(
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default6]:[rank6]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default6]:[rank6]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default6]:[rank6]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default6]:[rank6]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default6]:[rank6]:     Variable._execution_engine.run_backward(
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default6]:[rank6]:     return user_fn(self, *args)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py", line 622, in backward
retraining/0 [default6]:[rank6]:     gemm_out, *_, reduce_scatter_out = general_gemm(
retraining/0 [default6]:[rank6]:                                        ^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/cpp_extensions/gemm.py", line 113, in general_gemm
retraining/0 [default6]:[rank6]:     out, bias_grad, gelu_input, extra_output = tex.generic_gemm(*args, **kwargs)
retraining/0 [default6]:[rank6]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 6 has a total capacity of 79.25 GiB of which 32.94 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 73.22 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default4]:[rank4]: Traceback (most recent call last):
retraining/0 [default4]:[rank4]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default4]:[rank4]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default4]:[rank4]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default4]:[rank4]:     fdl_runner_app()
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default4]:[rank4]:     raise e
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default4]:[rank4]:     return get_command(self)(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default4]:[rank4]:     return self.main(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default4]:[rank4]:     return _main(
retraining/0 [default4]:[rank4]:            ^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default4]:[rank4]:     rv = self.invoke(ctx)
retraining/0 [default4]:[rank4]:          ^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default4]:[rank4]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default4]:[rank4]:     return __callback(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default4]:[rank4]:     return callback(**use_params)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default4]:[rank4]:     fdl_fn()
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default4]:[rank4]:     return train(
retraining/0 [default4]:[rank4]:            ^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default4]:[rank4]:     trainer.fit(model, data)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default4]:[rank4]:     call._call_and_handle_interrupt(
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default4]:[rank4]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default4]:[rank4]:     return function(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default4]:[rank4]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default4]:[rank4]:     results = self._run_stage()
retraining/0 [default4]:[rank4]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default4]:[rank4]:     self.fit_loop.run()
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default4]:[rank4]:     self.advance()
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default4]:[rank4]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default4]:[rank4]:     self.advance(data_fetcher)
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default4]:[rank4]:     super().advance(data_fetcher)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default4]:[rank4]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default4]:[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default4]:[rank4]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default4]:[rank4]:     call._call_lightning_module_hook(
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default4]:[rank4]:     output = fn(*args, **kwargs)
retraining/0 [default4]:[rank4]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default4]:[rank4]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default4]:[rank4]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default4]:[rank4]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default4]:[rank4]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default4]:[rank4]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default4]:[rank4]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default4]:[rank4]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default4]:[rank4]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default4]:[rank4]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default4]:[rank4]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default4]:[rank4]:     loss = closure()
retraining/0 [default4]:[rank4]:            ^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default4]:[rank4]:     closure_result = closure()
retraining/0 [default4]:[rank4]:                      ^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default4]:[rank4]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default4]:[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default4]:[rank4]:     return func(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default4]:[rank4]:     step_output = self._step_fn()
retraining/0 [default4]:[rank4]:                   ^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default4]:[rank4]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default4]:[rank4]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default4]:[rank4]:     output = fn(*args, **kwargs)
retraining/0 [default4]:[rank4]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default4]:[rank4]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default4]:[rank4]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default4]:[rank4]:     return self._step(
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default4]:[rank4]:     return self.forward(
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default4]:[rank4]:     microbatch_outputs = step()
retraining/0 [default4]:[rank4]:                          ^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default4]:[rank4]:     return self.forward_backward_func(
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default4]:[rank4]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default4]:[rank4]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default4]:[rank4]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default4]:[rank4]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default4]:[rank4]:     Variable._execution_engine.run_backward(
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default4]:[rank4]:     return user_fn(self, *args)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py", line 622, in backward
retraining/0 [default4]:[rank4]:     gemm_out, *_, reduce_scatter_out = general_gemm(
retraining/0 [default4]:[rank4]:                                        ^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/cpp_extensions/gemm.py", line 113, in general_gemm
retraining/0 [default4]:[rank4]:     out, bias_grad, gelu_input, extra_output = tex.generic_gemm(*args, **kwargs)
retraining/0 [default4]:[rank4]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 4 has a total capacity of 79.25 GiB of which 32.94 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 73.22 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default5]:[rank5]: Traceback (most recent call last):
retraining/0 [default5]:[rank5]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default5]:[rank5]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default5]:[rank5]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default5]:[rank5]:     fdl_runner_app()
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default5]:[rank5]:     raise e
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default5]:[rank5]:     return get_command(self)(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default5]:[rank5]:     return self.main(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default5]:[rank5]:     return _main(
retraining/0 [default5]:[rank5]:            ^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default5]:[rank5]:     rv = self.invoke(ctx)
retraining/0 [default5]:[rank5]:          ^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default5]:[rank5]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default5]:[rank5]:     return __callback(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default5]:[rank5]:     return callback(**use_params)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default5]:[rank5]:     fdl_fn()
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default5]:[rank5]:     return train(
retraining/0 [default5]:[rank5]:            ^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default5]:[rank5]:     trainer.fit(model, data)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default5]:[rank5]:     call._call_and_handle_interrupt(
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default5]:[rank5]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default5]:[rank5]:     return function(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default5]:[rank5]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default5]:[rank5]:     results = self._run_stage()
retraining/0 [default5]:[rank5]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default5]:[rank5]:     self.fit_loop.run()
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default5]:[rank5]:     self.advance()
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default5]:[rank5]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default5]:[rank5]:     self.advance(data_fetcher)
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default5]:[rank5]:     super().advance(data_fetcher)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default5]:[rank5]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default5]:[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default5]:[rank5]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default5]:[rank5]:     call._call_lightning_module_hook(
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default5]:[rank5]:     output = fn(*args, **kwargs)
retraining/0 [default5]:[rank5]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default5]:[rank5]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default5]:[rank5]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default5]:[rank5]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default5]:[rank5]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default5]:[rank5]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default5]:[rank5]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default5]:[rank5]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default5]:[rank5]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default5]:[rank5]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default5]:[rank5]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default5]:[rank5]:     loss = closure()
retraining/0 [default5]:[rank5]:            ^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default5]:[rank5]:     closure_result = closure()
retraining/0 [default5]:[rank5]:                      ^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default5]:[rank5]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default5]:[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default5]:[rank5]:     return func(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default5]:[rank5]:     step_output = self._step_fn()
retraining/0 [default5]:[rank5]:                   ^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default5]:[rank5]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default5]:[rank5]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default5]:[rank5]:     output = fn(*args, **kwargs)
retraining/0 [default5]:[rank5]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default5]:[rank5]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default5]:[rank5]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default5]:[rank5]:     return self._step(
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default5]:[rank5]:     return self.forward(
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default5]:[rank5]:     microbatch_outputs = step()
retraining/0 [default5]:[rank5]:                          ^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default5]:[rank5]:     return self.forward_backward_func(
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default5]:[rank5]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default5]:[rank5]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default5]:[rank5]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default5]:[rank5]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default5]:[rank5]:     Variable._execution_engine.run_backward(
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default5]:[rank5]:     return user_fn(self, *args)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py", line 622, in backward
retraining/0 [default5]:[rank5]:     gemm_out, *_, reduce_scatter_out = general_gemm(
retraining/0 [default5]:[rank5]:                                        ^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/cpp_extensions/gemm.py", line 113, in general_gemm
retraining/0 [default5]:[rank5]:     out, bias_grad, gelu_input, extra_output = tex.generic_gemm(*args, **kwargs)
retraining/0 [default5]:[rank5]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 5 has a total capacity of 79.25 GiB of which 32.94 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 73.22 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default7]:[rank7]: Traceback (most recent call last):
retraining/0 [default7]:[rank7]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default7]:[rank7]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default7]:[rank7]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default7]:[rank7]:     fdl_runner_app()
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default7]:[rank7]:     raise e
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default7]:[rank7]:     return get_command(self)(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default7]:[rank7]:     return self.main(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default7]:[rank7]:     return _main(
retraining/0 [default7]:[rank7]:            ^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default7]:[rank7]:     rv = self.invoke(ctx)
retraining/0 [default7]:[rank7]:          ^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default7]:[rank7]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default7]:[rank7]:     return __callback(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default7]:[rank7]:     return callback(**use_params)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default7]:[rank7]:     fdl_fn()
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default7]:[rank7]:     return train(
retraining/0 [default7]:[rank7]:            ^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default7]:[rank7]:     trainer.fit(model, data)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default7]:[rank7]:     call._call_and_handle_interrupt(
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default7]:[rank7]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default7]:[rank7]:     return function(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default7]:[rank7]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default7]:[rank7]:     results = self._run_stage()
retraining/0 [default7]:[rank7]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default7]:[rank7]:     self.fit_loop.run()
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default7]:[rank7]:     self.advance()
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default7]:[rank7]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default7]:[rank7]:     self.advance(data_fetcher)
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default7]:[rank7]:     super().advance(data_fetcher)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default7]:[rank7]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default7]:[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default7]:[rank7]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default7]:[rank7]:     call._call_lightning_module_hook(
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default7]:[rank7]:     output = fn(*args, **kwargs)
retraining/0 [default7]:[rank7]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default7]:[rank7]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default7]:[rank7]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default7]:[rank7]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default7]:[rank7]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default7]:[rank7]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default7]:[rank7]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default7]:[rank7]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default7]:[rank7]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default7]:[rank7]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default7]:[rank7]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default7]:[rank7]:     loss = closure()
retraining/0 [default7]:[rank7]:            ^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default7]:[rank7]:     closure_result = closure()
retraining/0 [default7]:[rank7]:                      ^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default7]:[rank7]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default7]:[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default7]:[rank7]:     return func(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default7]:[rank7]:     step_output = self._step_fn()
retraining/0 [default7]:[rank7]:                   ^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default7]:[rank7]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default7]:[rank7]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default7]:[rank7]:     output = fn(*args, **kwargs)
retraining/0 [default7]:[rank7]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default7]:[rank7]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default7]:[rank7]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default7]:[rank7]:     return self._step(
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default7]:[rank7]:     return self.forward(
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default7]:[rank7]:     microbatch_outputs = step()
retraining/0 [default7]:[rank7]:                          ^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default7]:[rank7]:     return self.forward_backward_func(
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default7]:[rank7]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default7]:[rank7]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default7]:[rank7]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default7]:[rank7]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default7]:[rank7]:     Variable._execution_engine.run_backward(
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default7]:[rank7]:     return user_fn(self, *args)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py", line 622, in backward
retraining/0 [default7]:[rank7]:     gemm_out, *_, reduce_scatter_out = general_gemm(
retraining/0 [default7]:[rank7]:                                        ^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/cpp_extensions/gemm.py", line 113, in general_gemm
retraining/0 [default7]:[rank7]:     out, bias_grad, gelu_input, extra_output = tex.generic_gemm(*args, **kwargs)
retraining/0 [default7]:[rank7]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 7 has a total capacity of 79.25 GiB of which 32.94 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 73.22 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
