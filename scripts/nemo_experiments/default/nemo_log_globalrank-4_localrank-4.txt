[NeMo W 2025-10-24 03:53:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:53:34 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:53:34 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:53:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 03:53:39 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 03:53:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 03:53:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:55:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:55:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:55:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:55:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:55:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:55:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 03:55:32 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 03:55:32 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 03:55:33 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:58:19 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:58:23 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Ranks 4 has data parallel rank: 4
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has model parallel group: [4]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has tensor model parallel group: [4]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo I 2025-10-24 03:58:25 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 03:58:26 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:58:29 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:01:22 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:01:25 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:01:25 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:01:26 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Ranks 4 has data parallel rank: 4
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has model parallel group: [4]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has tensor model parallel group: [4]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo I 2025-10-24 04:01:28 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:01:28 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:01:31 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:03:54 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:03:57 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:03:57 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:03:57 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:03:57 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:03:57 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:04:01 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:04:01 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:04:02 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:04:06 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:05:55 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:05:58 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:06:02 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:06:02 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:06:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:06:14 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:08:33 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:08:35 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:08:35 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:08:36 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:08:39 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:08:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:08:39 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:08:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:11:18 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:11:20 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:11:20 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:11:21 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:11:27 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:11:27 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:11:27 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:11:31 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:13:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:13:34 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:13:34 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:13:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:13:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:13:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:13:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:13:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:15:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:15:59 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:15:59 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:16:00 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:16:05 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:16:05 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:16:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:16:09 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:17:42 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:17:46 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:17:48 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:17:48 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:17:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:17:52 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:19:49 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:19:52 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:53 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:19:55 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:19:55 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:19:55 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:19:59 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:21:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:21:59 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:21:59 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:22:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:22:03 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:22:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:22:07 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:25:29 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:25:32 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:25:32 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:25:32 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:25:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:25:35 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:25:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:25:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:27:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:27:58 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:27:58 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:27:59 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:28:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:28:03 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:28:03 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:28:07 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:29:58 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:30:02 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:30:02 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:30:03 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:30:08 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:30:08 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:30:08 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:30:12 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:34:38 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:34:42 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:34:44 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:34:44 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:34:45 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:36:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:36:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:36:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:36:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:36:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:36:37 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:36:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:36:41 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:39:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:39:16 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:39:20 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:39:20 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:39:20 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:39:24 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:41:28 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:41:31 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:41:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:41:35 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:41:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:41:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:43:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:43:46 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:43:46 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:43:47 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:43:47 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:43:47 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has tensor model parallel rank: 4
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:43:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:43:52 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:43:52 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:43:52 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (4 ,0): 898895872
[NeMo W 2025-10-24 04:43:56 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:45:42 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:45:44 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:45:44 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:45:45 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has tensor model parallel rank: 4
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:46 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:45:49 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:45:49 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:45:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:45:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (4 ,0): 898895872
[NeMo W 2025-10-24 04:45:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:48:10 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:48:13 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has tensor model parallel rank: 4
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:48:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:48:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (4 ,0): 898895872
[NeMo W 2025-10-24 04:48:22 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:49:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:49:59 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:49:59 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:50:00 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:50:00 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:50:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has tensor model parallel rank: 4
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:50:06 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:50:06 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:50:06 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:50:06 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (4 ,0): 898895872
[NeMo W 2025-10-24 04:50:11 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:53:37 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:53:39 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:53:39 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:53:40 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has tensor model parallel rank: 4
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:53:43 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:53:43 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:53:43 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:53:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (4 ,0): 898895872
[NeMo W 2025-10-24 04:53:47 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:56:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:57:00 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has context parallel group: [4, 5]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has model parallel group: [4]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has tensor model parallel group: [4]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:57:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:57:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:57:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:57:12 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:59:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:59:16 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Ranks 4 has data parallel rank: 2
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has context parallel group: [4, 5]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has model parallel group: [4]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has tensor model parallel group: [4]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 04:59:20 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:59:20 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:59:20 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:59:30 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:03:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:03:25 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:03:25 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:03:26 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has context parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has model parallel group: [4]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has tensor model parallel group: [4]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:03:32 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:03:32 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 05:03:32 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:03:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:06:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:07:01 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:07:01 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:07:02 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has context parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has model parallel group: [4]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has tensor model parallel group: [4]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:07:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:07:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 05:07:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:07:15 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:13:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:13:47 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:13:47 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:13:48 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:13:48 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:13:48 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo W 2025-10-24 05:16:54 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:16:57 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:16:58 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:17:00 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:17:00 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:17:00 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:17:08 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:18:48 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:18:50 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:18:50 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:18:51 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:18:57 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:18:57 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:18:57 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:19:06 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:20:52 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:20:55 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:20:57 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:20:57 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:20:57 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:21:13 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:23:22 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:23:26 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:23:29 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:23:29 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:23:30 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:23:41 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:26:11 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:26:14 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:26:14 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:26:15 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:26:19 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:26:19 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:26:19 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:26:27 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:28:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:28:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:28:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:28:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:28:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:28:45 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:30:29 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:30:32 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has data parallel group : [0, 4]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Ranks 4 has data parallel rank: 1
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:30:36 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:30:36 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:30:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:30:44 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:33:12 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:33:14 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:33:14 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:33:15 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Ranks 4 has context parallel rank: 2
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:34:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:34:25 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:34:25 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:34:26 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Ranks 4 has context parallel rank: 2
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:34:31 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:34:31 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:34:31 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:34:31 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:34:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:36:33 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:36:35 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:36:35 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:36:36 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Ranks 4 has context parallel rank: 2
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:36:40 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:36:40 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:36:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:36:40 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:36:52 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:39:26 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:39:28 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:39:28 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:39:29 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Ranks 4 has context parallel rank: 2
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:39:33 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:39:33 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:39:33 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:39:33 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:39:49 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:42:10 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:42:13 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:42:13 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:42:14 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Ranks 4 has context parallel rank: 2
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:42:14 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:42:16 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:42:16 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:42:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:42:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:42:28 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:44:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:44:32 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:44:32 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:44:33 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Ranks 4 has context parallel rank: 2
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:44:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:44:35 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:44:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:44:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:44:45 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:48:42 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:48:44 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:48:44 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:48:45 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has context parallel group: [0, 4]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Ranks 4 has context parallel rank: 1
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:48:51 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:48:51 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:48:51 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:48:51 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1797001216
[NeMo W 2025-10-24 05:49:02 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:51:15 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:51:18 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has context parallel group: [0, 4]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Ranks 4 has context parallel rank: 1
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:51:22 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:51:22 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:51:22 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:51:22 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1797001216
[NeMo W 2025-10-24 05:51:33 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:53:36 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:53:39 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has context parallel group: [0, 4]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Ranks 4 has context parallel rank: 1
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:53:43 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:53:43 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:53:43 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:53:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1797001216
[NeMo W 2025-10-24 05:53:54 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:57:47 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:57:50 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has data parallel group : [4]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [0, 4]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has context parallel group: [0, 4]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Ranks 4 has context parallel rank: 1
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has pipeline model parallel group: [4]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has embedding group: [4]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 4 has embedding rank: 0
[NeMo W 2025-10-24 05:57:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:57:52 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:57:53 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:57:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 1797001216
[NeMo W 2025-10-24 05:58:01 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:01:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:01:15 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:01:15 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:01:16 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:01:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:01:17 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:01:18 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:01:18 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:01:18 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (1744965632 elements, 1744965632 padded size):
    	module.decoder.layers.14.mlp.linear_fc2.weight
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.13.self_attention.linear_proj.weight
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.9.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.8.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_proj.weight
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.12.self_attention.linear_qkv.weight
    	module.decoder.layers.11.mlp.linear_fc1.weight
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.13.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.8.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.13.mlp.linear_fc2.weight
    	module.decoder.layers.9.self_attention.linear_qkv.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.12.self_attention.linear_proj.weight
    	module.decoder.layers.10.mlp.linear_fc1.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.8.mlp.linear_fc2.weight
    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.15.mlp.linear_fc2.weight
    	module.decoder.layers.11.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.14.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.10.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.9.self_attention.linear_proj.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.12.mlp.linear_fc1.weight
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.13.self_attention.linear_qkv.weight
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.14.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.12.mlp.linear_fc2.weight
    	module.decoder.layers.11.mlp.linear_fc2.weight
    	module.decoder.layers.8.self_attention.linear_qkv.weight
    	module.decoder.layers.9.mlp.linear_fc1.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
[NeMo W 2025-10-24 06:01:23 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:03:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:03:48 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:03:48 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:03:49 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:03:54 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:03:54 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:03:55 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:03:55 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:03:55 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (1744965632 elements, 1744965632 padded size):
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_proj.weight
    	module.decoder.layers.11.mlp.linear_fc1.weight
    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.12.self_attention.linear_qkv.weight
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.13.mlp.linear_fc1.weight
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.11.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.8.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.13.mlp.linear_fc2.weight
    	module.decoder.layers.10.mlp.linear_fc2.weight
    	module.decoder.layers.9.self_attention.linear_qkv.weight
    	module.decoder.layers.12.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.8.mlp.linear_fc2.weight
    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.15.mlp.linear_fc2.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.14.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.9.self_attention.linear_proj.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.11.self_attention.linear_qkv.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.12.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.13.self_attention.linear_qkv.weight
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.14.mlp.linear_fc1.weight
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.12.mlp.linear_fc2.weight
    	module.decoder.layers.8.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.14.mlp.linear_fc2.weight
    	module.decoder.layers.10.mlp.linear_fc1.weight
    	module.decoder.layers.13.self_attention.linear_proj.weight
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.9.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.8.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
[NeMo W 2025-10-24 06:04:00 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:11:26 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:11:29 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:11:33 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:11:33 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:11:34 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:11:34 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:11:34 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (1744965632 elements, 1744965632 padded size):
    	module.decoder.layers.14.self_attention.linear_proj.weight
    	module.decoder.layers.10.mlp.linear_fc1.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.8.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.11.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.12.mlp.linear_fc1.weight
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.10.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.9.self_attention.linear_proj.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.13.self_attention.linear_qkv.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.14.mlp.linear_fc1.weight
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.12.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.weight
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.14.mlp.linear_fc2.weight
    	module.decoder.layers.8.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.13.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.15.self_attention.linear_proj.weight
    	module.decoder.layers.11.mlp.linear_fc1.weight
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.9.mlp.linear_fc2.weight
    	module.decoder.layers.8.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.12.self_attention.linear_qkv.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.13.mlp.linear_fc1.weight
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.11.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.14.self_attention.linear_qkv.weight
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.13.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.12.self_attention.linear_proj.weight
    	module.decoder.layers.8.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc2.weight
    	module.decoder.layers.9.self_attention.linear_qkv.weight
[NeMo W 2025-10-24 06:11:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:15:40 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:15:42 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:15:42 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:15:43 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:15:47 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:15:47 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:15:48 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:15:48 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:15:48 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (1744965632 elements, 1744965632 padded size):
    	module.decoder.layers.13.mlp.linear_fc2.weight
    	module.decoder.layers.9.self_attention.linear_qkv.weight
    	module.decoder.layers.12.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.8.mlp.linear_fc2.weight
    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.14.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.9.self_attention.linear_proj.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.11.self_attention.linear_qkv.weight
    	module.decoder.layers.10.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.12.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.13.self_attention.linear_qkv.weight
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.14.mlp.linear_fc1.weight
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.12.mlp.linear_fc2.weight
    	module.decoder.layers.8.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.11.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.14.mlp.linear_fc2.weight
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.13.self_attention.linear_proj.weight
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.9.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.8.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_proj.weight
    	module.decoder.layers.11.mlp.linear_fc1.weight
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.12.self_attention.linear_qkv.weight
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.13.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.11.mlp.linear_fc2.weight
    	module.decoder.layers.10.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.8.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
[NeMo W 2025-10-24 06:15:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:18:20 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:18:23 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:18:23 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:18:24 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:18:30 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:18:30 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:18:30 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:18:30 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:18:30 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (1744965632 elements, 1744965632 padded size):
    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.15.mlp.linear_fc2.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.14.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.10.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.9.self_attention.linear_proj.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.12.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.13.self_attention.linear_qkv.weight
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.14.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.12.mlp.linear_fc2.weight
    	module.decoder.layers.8.self_attention.linear_qkv.weight
    	module.decoder.layers.9.mlp.linear_fc1.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.14.mlp.linear_fc2.weight
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.13.self_attention.linear_proj.weight
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.9.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.8.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_proj.weight
    	module.decoder.layers.11.mlp.linear_fc1.weight
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.12.self_attention.linear_qkv.weight
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.13.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.11.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.8.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_qkv.weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.11.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.13.mlp.linear_fc2.weight
    	module.decoder.layers.9.self_attention.linear_qkv.weight
    	module.decoder.layers.12.self_attention.linear_proj.weight
    	module.decoder.layers.10.mlp.linear_fc1.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.8.mlp.linear_fc2.weight
[NeMo W 2025-10-24 06:18:35 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:20:03 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:20:05 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:20:05 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:20:06 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:20:10 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:20:10 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:20:10 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:20:11 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:20:12 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:20:12 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:20:12 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (872480768 elements, 872480768 padded size):
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
[NeMo I 2025-10-24 06:20:12 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (872484864 elements, 872484864 padded size):
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[NeMo W 2025-10-24 06:21:28 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:21:31 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:21:31 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:21:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:21:35 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:21:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:21:36 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:21:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:21:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:21:36 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (872480768 elements, 872480768 padded size):
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
[NeMo I 2025-10-24 06:21:36 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (872484864 elements, 872484864 padded size):
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[NeMo W 2025-10-24 06:22:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:22:58 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:22:58 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:22:59 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:23:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:23:03 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:23:03 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:23:04 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:23:04 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:23:04 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:23:05 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (872480768 elements, 872480768 padded size):
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
[NeMo I 2025-10-24 06:23:05 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (872484864 elements, 872484864 padded size):
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
[NeMo W 2025-10-24 06:27:19 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:27:22 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:27:23 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:27:24 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:27:24 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:27:25 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:27:25 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:27:25 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (1744965632 elements, 1744965632 padded size):
    	module.decoder.layers.12.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.13.mlp.linear_fc2.weight
    	module.decoder.layers.9.self_attention.linear_qkv.weight
    	module.decoder.layers.15.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.14.self_attention.linear_qkv.weight
    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.8.mlp.linear_fc1.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.11.mlp.linear_fc2.weight
    	module.decoder.layers.10.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.13.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.12.self_attention.linear_qkv.weight
    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.10.self_attention.linear_proj.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.15.self_attention.linear_proj.weight
    	module.decoder.layers.11.mlp.linear_fc1.weight
    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.8.self_attention.linear_proj.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.9.mlp.linear_fc2.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.13.self_attention.linear_proj.weight
    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.14.mlp.linear_fc2.weight
    	module.decoder.layers.10.self_attention.linear_qkv.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.15.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.11.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc2.weight
    	module.decoder.layers.8.self_attention.linear_qkv.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.14.mlp.linear_fc1.weight
    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.13.self_attention.linear_qkv.weight
    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.12.mlp.linear_fc1.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.11.self_attention.linear_qkv.weight
    	module.decoder.layers.10.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.9.self_attention.linear_proj.weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.14.self_attention.linear_proj.weight
    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.15.mlp.linear_fc2.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.8.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
[NeMo W 2025-10-24 06:27:30 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:28:41 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:28:44 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:28:44 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:28:45 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:28:47 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:28:47 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:28:47 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:28:48 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:28:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:28:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:28:49 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (872480768 elements, 872480768 padded size):
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
[NeMo I 2025-10-24 06:28:49 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (872484864 elements, 872484864 padded size):
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
[NeMo W 2025-10-24 06:30:47 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:30:50 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:30:50 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:30:51 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:30:51 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:30:51 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:30:52 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:30:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:30:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:30:53 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (872480768 elements, 872480768 padded size):
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
[NeMo I 2025-10-24 06:30:53 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (872484864 elements, 872484864 padded size):
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
[NeMo W 2025-10-24 06:32:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:32:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:32:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:32:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Ranks 4 has data parallel rank: 0
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has context parallel group: [4]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Ranks 4 has context parallel rank: 0
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has pipeline model parallel group: [0, 4]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has embedding group: [0, 4]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 4 has embedding rank: 1
[NeMo W 2025-10-24 06:32:38 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:32:38 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:32:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:32:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:32:40 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:32:40 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,1): 1848250368
[NeMo I 2025-10-24 06:32:40 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
    Params for bucket 1 (872480768 elements, 872480768 padded size):
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
[NeMo I 2025-10-24 06:32:40 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 2
    Params for bucket 1 (103284736 elements, 103284736 padded size):
    	module.output_layer.weight
    Params for bucket 2 (872484864 elements, 872484864 padded size):
    	module.decoder.final_layernorm.weight
    	module.decoder.layers.4.mlp.linear_fc2.weight
    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.2.self_attention.linear_qkv.weight
    	module.decoder.layers.0.mlp.linear_fc2.weight
    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.7.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.2.mlp.linear_fc1.weight
    	module.decoder.layers.2.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.weight
    	module.decoder.layers.7.self_attention.linear_proj.weight
    	module.decoder.layers.5.self_attention.linear_qkv.weight
    	module.decoder.layers.3.mlp.linear_fc2.weight
    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.1.self_attention.linear_qkv.weight
    	module.decoder.layers.6.self_attention.linear_qkv.weight
    	module.decoder.layers.5.mlp.linear_fc1.weight
    	module.decoder.layers.5.self_attention.linear_proj.weight
    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.1.mlp.linear_fc1.weight
    	module.decoder.layers.1.self_attention.linear_proj.weight
    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc1.weight
    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.4.self_attention.linear_qkv.weight
    	module.decoder.layers.2.mlp.linear_fc2.weight
    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.0.self_attention.linear_qkv.weight
    	module.decoder.layers.7.mlp.linear_fc2.weight
    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.4.mlp.linear_fc1.weight
    	module.decoder.layers.4.self_attention.linear_proj.weight
    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.0.mlp.linear_fc1.weight
    	module.decoder.layers.0.self_attention.linear_proj.weight
    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.5.mlp.linear_fc2.weight
    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.3.self_attention.linear_qkv.weight
    	module.decoder.layers.1.mlp.linear_fc2.weight
    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
    	module.decoder.layers.6.mlp.linear_fc2.weight
    	module.decoder.layers.6.self_attention.linear_proj.weight
    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
    	module.decoder.layers.3.mlp.linear_fc1.weight
    	module.decoder.layers.3.self_attention.linear_proj.weight
    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
