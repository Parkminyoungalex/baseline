[NeMo W 2025-10-24 03:50:19 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:50:22 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:50:22 nemo_logging:393] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json to /root/.cache/torch/megatron/megatron-gpt-345m_vocab
[NeMo I 2025-10-24 03:50:23 nemo_logging:393] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt to /root/.cache/torch/megatron/megatron-gpt-345m_merges
[NeMo I 2025-10-24 03:50:25 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:50:26 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has data parallel group : [0, 1, 2, 3]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [0, 1, 2, 3]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3]]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Ranks 3 has data parallel rank: 3
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3]]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has model parallel group: [3]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3]]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has tensor model parallel group: [3]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3]]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3]]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3]]
[NeMo I 2025-10-24 03:50:26 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo I 2025-10-24 03:50:29 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 03:50:29 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:50:36 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 03:53:33 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:53:36 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:37 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 03:53:39 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 03:53:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 03:53:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:55:25 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:55:27 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:55:27 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:55:28 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:55:28 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:55:28 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 03:55:32 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 03:55:32 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 03:55:33 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:58:16 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:58:18 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:58:18 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:58:19 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Ranks 3 has data parallel rank: 3
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has model parallel group: [3]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has tensor model parallel group: [3]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:19 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo I 2025-10-24 03:58:25 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 03:58:25 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:58:29 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:01:22 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:01:25 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:01:25 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:01:26 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Ranks 3 has data parallel rank: 3
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has model parallel group: [3]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has tensor model parallel group: [3]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo I 2025-10-24 04:01:28 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:01:28 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:01:31 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:03:55 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:03:59 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:04:01 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:04:01 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:04:02 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:04:06 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:05:54 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:05:57 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:06:02 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:06:02 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:06:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:06:14 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:08:34 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:08:37 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:08:37 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:08:37 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:08:37 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:08:37 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:38 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:08:39 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:08:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:08:39 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:08:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:11:19 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:11:22 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:22 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:11:27 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:11:27 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:11:27 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:11:31 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:13:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:13:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:13:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:13:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:13:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:13:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:16:00 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:16:02 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:16:02 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:16:03 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:03 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:16:05 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:16:05 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:16:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:16:09 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:17:41 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:17:44 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:17:44 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:17:44 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:17:44 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:17:44 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:45 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:17:48 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:17:48 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:17:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:17:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:19:47 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:19:50 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:19:50 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:19:50 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:19:50 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:19:50 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:19:55 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:19:55 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:19:55 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:19:55 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 04:19:59 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:21:55 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:21:57 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:21:57 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:21:58 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:21:58 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:22:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:22:03 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:22:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:22:04 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 04:22:07 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:25:26 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:25:28 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:25:28 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:25:29 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:29 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:25:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:25:35 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:25:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:25:35 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 04:25:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:27:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:27:59 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:28:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:28:03 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:28:03 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:28:03 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 04:28:07 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:30:01 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:30:03 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:30:04 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:30:08 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:30:08 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:30:08 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:30:08 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 04:30:11 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:34:39 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:34:42 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:43 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:34:44 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:34:44 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:34:45 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:36:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:36:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:35 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:36:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:36:37 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:36:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:36:38 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 04:36:41 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:39:11 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:39:13 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:39:13 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:39:14 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:39:20 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:39:20 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:39:20 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:39:20 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 04:39:24 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:41:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:41:32 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:41:32 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:41:33 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:33 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:41:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:41:35 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:41:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:41:35 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 04:41:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:43:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:43:49 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:43:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:43:52 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:43:52 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:43:52 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 898895872
[NeMo W 2025-10-24 04:43:56 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:45:42 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:45:44 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:45:44 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:45:45 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:45:49 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:45:49 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:45:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:45:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 898895872
[NeMo W 2025-10-24 04:45:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:48:11 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:48:14 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:15 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:48:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:48:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 898895872
[NeMo W 2025-10-24 04:48:22 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:49:58 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:50:02 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:50:06 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:50:06 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:50:06 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:50:06 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 898895872
[NeMo W 2025-10-24 04:50:11 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:53:37 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:53:39 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:53:39 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:53:40 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:53:43 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:53:43 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:53:43 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:53:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 898895872
[NeMo W 2025-10-24 04:53:47 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:56:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:57:00 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:57:01 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has context parallel group: [2, 3]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has model parallel group: [3]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has tensor model parallel group: [3]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:57:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:57:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:57:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:57:12 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:59:12 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:59:14 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:59:14 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:59:15 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has context parallel group: [2, 3]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has model parallel group: [3]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has tensor model parallel group: [3]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:15 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 04:59:20 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:59:20 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:59:20 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:59:30 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:03:23 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:03:26 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:03:27 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has context parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Ranks 3 has context parallel rank: 3
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has model parallel group: [3]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has tensor model parallel group: [3]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:03:32 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:03:32 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 05:03:32 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:03:32 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 7185633280
[NeMo W 2025-10-24 05:03:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:06:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:07:01 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:07:01 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:07:02 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has context parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Ranks 3 has context parallel rank: 3
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has model parallel group: [3]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has tensor model parallel group: [3]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:07:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:07:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 05:07:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:07:05 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 7185633280
[NeMo W 2025-10-24 05:07:15 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:13:42 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:13:44 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:13:44 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:13:45 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:13:45 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:13:45 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has context parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Ranks 3 has context parallel rank: 3
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has model parallel group: [3]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has tensor model parallel group: [3]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:16:54 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:16:57 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has context parallel group: [1, 3]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:17:00 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:17:00 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:17:00 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:17:00 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:17:08 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:18:48 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:18:50 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:18:50 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:18:51 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has context parallel group: [1, 3]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:18:57 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:18:57 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:18:57 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:18:57 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:19:06 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:20:51 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:20:53 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:20:53 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:20:54 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has context parallel group: [1, 3]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:20:57 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:20:57 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:20:57 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:20:57 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:21:13 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:23:25 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:23:28 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has context parallel group: [1, 3]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:23:28 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:23:29 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:23:29 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:23:30 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:23:30 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:23:41 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:26:14 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:26:17 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:26:17 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:26:18 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has context parallel group: [1, 3]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:26:18 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:26:19 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:26:19 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:26:19 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:26:19 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:26:27 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:28:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:28:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has context parallel group: [1, 3]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:28:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:28:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:28:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:28:38 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:28:45 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:30:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:30:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:30:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:30:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has data parallel group : [3, 7]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has context parallel group: [1, 3]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:30:35 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:30:36 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:30:36 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:30:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:30:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:30:44 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:33:11 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:33:14 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:33:14 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:33:15 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:33:15 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:34:25 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:34:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:34:30 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:34:31 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:34:31 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:34:31 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:34:31 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:34:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:36:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:36:32 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:36:32 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:36:33 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:36:33 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:36:40 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:36:40 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:36:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:36:40 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:36:52 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:39:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:39:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:39:33 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:39:33 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:39:33 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:39:33 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:39:49 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:42:09 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:42:12 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:42:12 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:42:16 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:42:16 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:42:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:42:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:42:28 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:44:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:44:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Ranks 3 has context parallel rank: 1
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has model parallel group: [2, 3]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:44:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:44:35 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:44:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:44:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:44:45 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:48:43 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:48:46 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has context parallel group: [3, 7]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:48:51 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:48:51 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:48:51 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:48:51 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 05:49:02 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:51:15 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:51:17 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:51:17 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:51:18 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has context parallel group: [3, 7]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:51:22 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:51:22 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:51:22 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:51:22 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 05:51:33 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:53:36 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:53:39 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has context parallel group: [3, 7]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:53:39 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:53:43 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:53:43 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:53:43 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:53:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 05:53:54 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:57:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:57:49 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has data parallel group : [3]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [3, 7]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Ranks 3 has data parallel rank: 0
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has context parallel group: [3, 7]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has tensor model parallel group: [0, 1, 2, 3]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has tensor model parallel rank: 3
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has pipeline model parallel group: [3]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has embedding group: [3]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 05:57:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:57:52 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:57:53 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:57:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (3 ,0): 1797001216
[NeMo W 2025-10-24 05:58:01 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:01:09 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:01:11 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:01:11 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:01:12 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:01:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:01:17 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:01:18 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:01:24 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:03:50 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:03:52 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:03:52 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:03:53 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:03:53 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:03:54 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:03:54 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:03:55 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:04:00 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:11:25 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:11:27 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:11:27 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:11:28 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:11:28 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:11:33 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:11:33 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:11:34 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:11:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:15:40 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:15:44 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:15:47 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:15:47 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:15:48 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:15:54 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:18:25 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:18:27 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:18:27 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:18:28 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:18:28 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:18:30 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:18:30 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:18:30 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:18:36 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:20:05 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:20:07 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:20:07 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:20:08 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:20:08 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:20:10 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:20:10 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:20:12 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:20:13 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:21:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:21:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:21:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:21:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:21:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:21:35 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:21:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:21:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:22:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:22:58 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:22:58 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:22:59 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:22:59 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:23:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:23:03 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:23:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:23:06 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:27:17 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:27:20 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:27:20 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:27:21 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:27:24 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:27:24 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:27:25 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:27:30 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:28:41 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:28:43 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:28:43 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:28:44 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:28:44 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:28:44 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:28:47 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:28:47 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:28:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:28:50 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:30:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:30:49 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:30:49 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:30:51 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:30:51 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:30:53 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:30:54 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:32:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:32:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:32:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:32:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has data parallel group : [1, 3]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has combined group of data parallel and context parallel : [1, 3]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Ranks 3 has data parallel rank: 1
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has context parallel group: [3]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Ranks 3 has context parallel rank: 0
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has tensor model parallel group: [2, 3]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has pipeline model parallel group: [3, 7]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has embedding group: [3, 7]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has pipeline model parallel rank 0
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:32:34 nemo_logging:393] Rank 3 has embedding rank: 0
[NeMo W 2025-10-24 06:32:38 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:32:38 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:32:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:32:41 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
