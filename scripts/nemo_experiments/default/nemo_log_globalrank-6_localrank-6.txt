[NeMo W 2025-10-24 03:53:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:53:36 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:36 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 03:53:39 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 03:53:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 03:53:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:55:25 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:55:27 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:55:27 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:55:28 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:55:28 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:55:28 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 03:55:32 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 03:55:32 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 03:55:33 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:58:18 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:58:21 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:58:21 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:58:21 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:58:21 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:58:21 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Ranks 6 has data parallel rank: 6
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has model parallel group: [6]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has tensor model parallel group: [6]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo I 2025-10-24 03:58:25 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 03:58:25 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:58:29 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:01:21 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:01:23 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:01:23 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:01:24 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Ranks 6 has data parallel rank: 6
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has model parallel group: [6]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has tensor model parallel group: [6]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:24 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo I 2025-10-24 04:01:28 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:01:28 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:01:31 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:03:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:03:59 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:04:00 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:04:00 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:04:01 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:04:01 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:04:02 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:04:06 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:05:54 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:05:56 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:05:56 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:05:57 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:06:02 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:06:02 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:06:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:06:14 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:08:33 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:08:35 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:08:35 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:08:36 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:36 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:08:39 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:08:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:08:39 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:08:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:11:16 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:11:20 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:11:20 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:11:20 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:11:20 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:11:20 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:11:27 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:11:27 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:11:27 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:11:31 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:13:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:13:34 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:13:34 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:13:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:13:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:13:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:13:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:13:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:15:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:15:59 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:15:59 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:16:00 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:16:05 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:16:05 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:16:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:16:09 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:17:39 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:17:41 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:17:41 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:17:42 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:42 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:17:48 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:17:48 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:17:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:17:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:19:49 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:19:51 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:19:52 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:52 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:19:55 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:19:55 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:19:55 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:19:59 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:21:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:22:00 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:22:00 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:22:01 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:22:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:22:03 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:22:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:22:07 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:25:29 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:25:32 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:25:32 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:25:32 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:33 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:25:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:25:35 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:25:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:25:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:27:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:28:00 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:01 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:28:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:28:03 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:28:03 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:28:07 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:30:03 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:30:06 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:30:06 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:30:06 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:30:06 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:30:06 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:07 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:30:08 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:30:08 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:30:08 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:30:12 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:34:37 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:34:40 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:34:40 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:34:41 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:34:44 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:34:44 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:34:45 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:36:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:36:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:36:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:36:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:36:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:36:37 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:36:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:36:41 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:39:14 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:39:16 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:39:17 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:17 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:39:20 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:39:20 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:39:20 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:39:24 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:41:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:41:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:31 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:41:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:41:35 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:41:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:41:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:43:47 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:43:49 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:43:50 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [6]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has tensor model parallel rank: 6
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:50 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:43:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:43:52 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:43:52 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:43:52 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (6 ,0): 898895872
[NeMo W 2025-10-24 04:43:56 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:45:41 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:45:44 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:45:44 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:45:44 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [6]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has tensor model parallel rank: 6
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:45 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:45:49 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:45:49 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:45:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:45:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (6 ,0): 898895872
[NeMo W 2025-10-24 04:45:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:48:09 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:48:11 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:48:11 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:48:12 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [6]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has tensor model parallel rank: 6
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:48:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:48:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (6 ,0): 898895872
[NeMo W 2025-10-24 04:48:22 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:49:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:50:02 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:50:02 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [6]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has tensor model parallel rank: 6
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:03 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:50:06 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:50:06 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:50:06 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:50:06 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (6 ,0): 898895872
[NeMo W 2025-10-24 04:50:11 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:53:37 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:53:40 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [6]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has tensor model parallel rank: 6
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:53:43 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:53:43 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:53:43 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:53:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (6 ,0): 898895872
[NeMo W 2025-10-24 04:53:47 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:56:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:57:01 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:57:02 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has context parallel group: [6, 7]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has model parallel group: [6]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has tensor model parallel group: [6]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:57:02 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:57:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:57:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:57:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:57:12 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:59:14 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:59:17 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has data parallel group : [0, 2, 4, 6]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Ranks 6 has data parallel rank: 3
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has context parallel group: [6, 7]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has model parallel group: [6]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has tensor model parallel group: [6]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:18 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 04:59:20 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:59:20 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:59:20 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:59:30 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:03:24 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:03:27 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:03:27 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has context parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Ranks 6 has context parallel rank: 2
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has model parallel group: [6]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has tensor model parallel group: [6]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:28 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:03:32 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:03:32 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 05:03:32 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:03:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:06:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:07:03 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has context parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Ranks 6 has context parallel rank: 2
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has model parallel group: [6]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has tensor model parallel group: [6]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:03 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:07:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:07:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 05:07:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:07:15 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:13:42 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:13:45 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:13:45 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:13:46 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:13:46 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo W 2025-10-24 05:16:53 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:16:55 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:16:55 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:16:56 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:17:00 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:17:00 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:17:00 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:17:08 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:18:51 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:18:53 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:18:53 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:18:54 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:18:54 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:18:57 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:18:57 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:18:57 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:19:06 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:20:52 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:20:55 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:20:57 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:20:57 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:20:57 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:21:13 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:23:22 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:23:24 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:23:24 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:23:25 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:23:25 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:23:29 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:23:29 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:23:30 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:23:41 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:26:12 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:26:14 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:26:14 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:26:15 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:26:19 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:26:19 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:26:19 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:26:27 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:28:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:28:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:28:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:28:36 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:28:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:28:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:28:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:28:45 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:30:29 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:30:32 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:30:33 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has data parallel group : [2, 6]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has context parallel group: [4, 6]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:30:33 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:30:36 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:30:36 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:30:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:30:44 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:33:10 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:33:13 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Ranks 6 has context parallel rank: 3
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:33:13 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:34:25 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:34:28 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:34:28 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:34:29 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Ranks 6 has context parallel rank: 3
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:34:29 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:34:31 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:34:31 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:34:31 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:34:31 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:34:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:36:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:36:35 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:36:35 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:36:36 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Ranks 6 has context parallel rank: 3
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:36:40 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:36:40 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:36:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:36:40 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:36:52 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:39:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:39:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Ranks 6 has context parallel rank: 3
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:39:33 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:39:33 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:39:33 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:39:33 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:39:49 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:42:07 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:42:10 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:42:10 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:42:10 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:42:10 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:42:10 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Ranks 6 has context parallel rank: 3
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:42:16 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:42:16 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:42:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:42:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:42:28 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:44:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:44:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:44:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [0, 2, 4, 6]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has context parallel group: [0, 2, 4, 6]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Ranks 6 has context parallel rank: 3
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has model parallel group: [6, 7]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:44:34 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:44:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:44:35 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:44:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:44:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 3593211904
[NeMo W 2025-10-24 05:44:45 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:48:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:48:47 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has context parallel group: [2, 6]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:48:47 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:48:51 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:48:51 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:48:51 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:48:51 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (2 ,0): 1797001216
[NeMo W 2025-10-24 05:49:02 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:51:16 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:51:18 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:51:19 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has context parallel group: [2, 6]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:51:22 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:51:22 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:51:22 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:51:22 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (2 ,0): 1797001216
[NeMo W 2025-10-24 05:51:33 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:53:36 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:53:40 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:53:40 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:53:41 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has context parallel group: [2, 6]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:53:41 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:53:43 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:53:43 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:53:43 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:53:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (2 ,0): 1797001216
[NeMo W 2025-10-24 05:53:54 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:57:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:57:48 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:57:48 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:57:49 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has data parallel group : [6]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [2, 6]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Ranks 6 has data parallel rank: 0
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has context parallel group: [2, 6]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Ranks 6 has context parallel rank: 1
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has tensor model parallel rank: 2
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has pipeline model parallel group: [6]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has embedding group: [6]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Rank 6 has embedding rank: 0
[NeMo W 2025-10-24 05:57:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:57:52 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:57:53 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:57:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (2 ,0): 1797001216
[NeMo W 2025-10-24 05:58:01 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:01:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:01:15 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:01:15 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:01:16 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:01:16 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:01:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:01:17 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:01:18 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:01:23 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:03:47 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:03:50 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:03:50 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:03:54 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:03:54 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:03:55 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:04:00 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:11:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:11:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:11:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:11:33 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:11:33 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:11:34 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:11:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:15:41 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:15:44 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:15:47 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:15:47 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:15:48 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:15:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:18:22 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:18:25 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:18:30 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:18:30 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:18:30 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:18:35 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:20:03 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:20:05 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:20:05 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:20:06 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:20:10 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:20:10 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:20:10 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:20:11 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:21:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:21:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:21:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:21:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:21:30 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:21:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:21:35 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:21:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:21:36 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:22:58 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:23:02 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:23:02 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:23:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:23:03 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:23:03 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:23:04 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:27:19 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:27:22 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:27:22 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:27:24 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:27:24 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:27:25 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 06:27:30 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:28:43 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:28:46 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:28:46 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:28:47 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:28:47 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:28:47 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:28:48 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:30:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:30:46 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:30:46 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:30:47 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:30:51 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:30:51 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:30:51 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:30:52 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo W 2025-10-24 06:32:34 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:32:37 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has data parallel group : [4, 6]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has combined group of data parallel and context parallel : [4, 6]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Ranks 6 has data parallel rank: 1
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has context parallel group: [6]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Ranks 6 has context parallel rank: 0
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has model parallel group: [2, 3, 6, 7]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has tensor model parallel group: [6, 7]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has tensor model parallel rank: 0
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has pipeline model parallel group: [2, 6]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has embedding group: [2, 6]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:32:37 nemo_logging:393] Rank 6 has embedding rank: 1
[NeMo W 2025-10-24 06:32:38 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:32:38 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:32:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:32:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
