[NeMo W 2025-10-24 03:53:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:53:34 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:53:34 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:53:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:53:35 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 03:53:39 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 03:53:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 03:53:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:55:28 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:55:30 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:55:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:55:31 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:55:31 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 03:55:32 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 03:55:32 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 03:55:33 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:58:20 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 03:58:22 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 03:58:23 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 03:58:23 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-10-24 03:58:25 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 03:58:26 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 03:58:29 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:01:22 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:01:25 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:01:25 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:01:26 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has data parallel group : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Ranks 5 has data parallel rank: 5
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:01:26 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo I 2025-10-24 04:01:28 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:01:28 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:01:31 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:03:55 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:03:57 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:03:57 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:03:58 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:03:58 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:04:01 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:04:01 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:04:02 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:04:06 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:05:55 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:05:57 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:05:58 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:05:58 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:06:01 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:06:02 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:06:02 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:06:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:06:14 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:08:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:08:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:08:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:08:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:08:34 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:08:39 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:08:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:08:39 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:08:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:11:17 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:11:19 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:11:19 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:11:20 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:11:20 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:11:20 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:11:21 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:11:27 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:11:27 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:11:27 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:11:31 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:13:32 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:13:34 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:13:34 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:13:35 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:13:35 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:13:36 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:13:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:13:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:13:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:13:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:15:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:15:59 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:15:59 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:16:00 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:16:00 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:16:05 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:16:05 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:16:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:16:09 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:17:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:17:46 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:17:47 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:17:47 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:17:48 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:17:48 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:17:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:17:52 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:19:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:19:46 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:19:46 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:19:47 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:19:47 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:19:55 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:19:55 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:19:55 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:19:59 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:21:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:22:01 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:22:02 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:22:02 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:22:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:22:03 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:22:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:22:07 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:25:28 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:25:30 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:25:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:25:31 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:25:31 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:25:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:25:35 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:25:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:25:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:27:57 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:27:59 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:28:00 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:28:00 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:28:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:28:03 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:28:03 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:28:07 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:30:01 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:30:04 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:30:04 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:30:05 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:30:08 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:30:08 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:30:08 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:30:12 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:34:39 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:34:41 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:34:42 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:34:42 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:34:44 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:34:44 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 04:34:45 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:36:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:36:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:36:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:36:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:36:34 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:36:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:36:37 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:36:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:36:41 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:39:12 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:39:14 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:39:15 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:39:15 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:39:20 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:39:20 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:39:20 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:39:24 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:41:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:41:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:41:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:41:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:41:30 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:41:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:41:35 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 04:41:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:41:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:43:45 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:43:47 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:43:47 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:43:48 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has tensor model parallel rank: 5
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:43:48 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:43:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:43:52 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:43:52 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:43:52 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (5 ,0): 898895872
[NeMo W 2025-10-24 04:43:56 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:45:40 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:45:42 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:45:42 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:45:43 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has tensor model parallel rank: 5
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:45:43 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:45:49 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:45:49 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:45:49 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:45:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (5 ,0): 898895872
[NeMo W 2025-10-24 04:45:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:48:10 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:48:12 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:48:13 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has tensor model parallel rank: 5
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:48:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:48:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (5 ,0): 898895872
[NeMo W 2025-10-24 04:48:22 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:49:58 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:50:00 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:50:00 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:50:01 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has tensor model parallel rank: 5
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:50:01 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:50:06 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:50:06 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:50:06 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:50:06 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (5 ,0): 898895872
[NeMo W 2025-10-24 04:50:11 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:53:37 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:53:39 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:53:39 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:53:40 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:53:40 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has tensor model parallel rank: 5
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:53:41 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:53:43 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:53:43 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
[NeMo I 2025-10-24 04:53:43 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 04:53:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (5 ,0): 898895872
[NeMo W 2025-10-24 04:53:47 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:56:56 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:56:58 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:56:58 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:56:59 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has context parallel group: [4, 5]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:56:59 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:57:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:57:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:57:04 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:57:12 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 04:59:13 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 04:59:16 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 04:59:16 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has data parallel group : [1, 3, 5, 7]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Ranks 5 has data parallel rank: 2
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has context parallel group: [4, 5]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] All context parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 04:59:17 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 04:59:20 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 04:59:20 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 04:59:20 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 04:59:30 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:03:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:03:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:03:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:03:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has context parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:03:30 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:03:32 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:03:32 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 05:03:32 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:03:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:06:58 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:07:00 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:07:00 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:07:01 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:07:01 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:07:01 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has context parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has model parallel group: [5]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has tensor model parallel group: [5]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has tensor model parallel rank: 0
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:07:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:07:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
[NeMo I 2025-10-24 05:07:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:07:15 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:13:42 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:13:44 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:13:44 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:13:45 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:13:45 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:13:45 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo W 2025-10-24 05:16:54 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:16:56 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:16:57 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has context parallel group: [5, 7]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:16:57 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:17:00 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:17:00 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:17:00 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:17:08 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:18:49 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:18:51 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:18:52 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has context parallel group: [5, 7]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:18:52 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:18:57 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:18:57 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:18:57 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:19:06 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:20:52 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:20:54 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:20:55 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:20:55 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has context parallel group: [5, 7]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:20:56 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:20:57 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:20:57 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:20:57 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:21:13 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:23:24 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:23:26 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:23:27 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has context parallel group: [5, 7]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:23:27 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:23:29 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:23:29 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:23:30 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:23:41 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:26:12 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:26:14 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:26:14 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:26:15 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:26:15 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has context parallel group: [5, 7]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:26:16 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:26:19 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:26:19 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:26:19 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:26:27 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:28:31 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:28:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:28:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:28:34 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has context parallel group: [5, 7]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:28:34 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:28:37 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:28:37 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:28:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:28:45 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:30:28 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:30:30 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:30:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:30:31 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has data parallel group : [1, 5]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Ranks 5 has data parallel rank: 1
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has context parallel group: [5, 7]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] All context parallel group ranks: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:30:31 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:30:36 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:30:36 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:30:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo W 2025-10-24 05:30:44 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:33:09 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:33:11 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:33:11 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:33:12 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Ranks 5 has context parallel rank: 2
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:33:12 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:34:24 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:34:26 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:34:27 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Ranks 5 has context parallel rank: 2
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:34:27 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:34:31 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:34:31 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:34:32 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:34:32 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:34:43 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:36:34 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:36:36 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:36:37 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Ranks 5 has context parallel rank: 2
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:36:37 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:36:40 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:36:40 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:36:40 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:36:40 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:36:52 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:39:26 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:39:29 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:39:30 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Ranks 5 has context parallel rank: 2
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:39:30 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:39:33 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:39:33 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:39:33 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:39:33 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:39:49 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:42:08 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:42:10 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:42:10 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:42:11 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Ranks 5 has context parallel rank: 2
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:42:11 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:42:16 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:42:16 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:42:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:42:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:42:28 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:44:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:44:30 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:44:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:44:31 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 3, 5, 7]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has context parallel group: [1, 3, 5, 7]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] All context parallel group ranks: [[0, 2, 4, 6], [1, 3, 5, 7]]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Ranks 5 has context parallel rank: 2
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has model parallel group: [4, 5]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] All model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:44:31 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:44:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:44:35 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 05:44:36 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:44:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 3593211904
[NeMo W 2025-10-24 05:44:45 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:48:43 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:48:45 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:48:46 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:48:46 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:48:51 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:48:51 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:48:51 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:48:51 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 1797001216
[NeMo W 2025-10-24 05:49:02 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:51:17 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:51:19 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:51:20 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:51:20 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:51:22 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:51:22 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:51:22 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:51:22 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 1797001216
[NeMo W 2025-10-24 05:51:33 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:53:35 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:53:37 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:53:37 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:53:38 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:53:38 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:53:43 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:53:43 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:53:43 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:53:43 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 1797001216
[NeMo W 2025-10-24 05:53:54 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 05:57:47 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 05:57:49 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 05:57:50 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has data parallel group : [5]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [1, 5]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has context parallel group: [1, 5]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All context parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Ranks 5 has context parallel rank: 1
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5, 6, 7]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has pipeline model parallel group: [5]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has embedding group: [5]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has pipeline model parallel rank 0
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 05:57:50 nemo_logging:393] Rank 5 has embedding rank: 0
[NeMo W 2025-10-24 05:57:52 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 05:57:52 nemo_logging:393] Padded vocab_size: 50688, original vocab_size: 50257, dummy tokens: 431.
[NeMo I 2025-10-24 05:57:53 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 05:57:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,0): 1797001216
[NeMo W 2025-10-24 05:58:01 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:01:10 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:01:12 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:01:13 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:01:13 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:01:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:01:17 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:01:18 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:01:18 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:01:23 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:03:46 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:03:48 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:03:48 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:03:49 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:03:49 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:03:54 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:03:54 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:03:55 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:03:55 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:04:00 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:11:28 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:11:30 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:11:31 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:11:31 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:11:33 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:11:33 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:11:34 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:11:34 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:11:39 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:15:40 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:15:43 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:15:44 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:15:44 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:15:47 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:15:47 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:15:48 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:15:48 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:15:53 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:18:21 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:18:24 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:18:24 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:18:25 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:18:30 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:18:30 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:18:30 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:18:30 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:18:35 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:20:02 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:20:05 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:20:05 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:20:05 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:20:05 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:20:05 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:20:06 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:20:10 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:20:10 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:20:10 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:20:11 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:20:12 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo I 2025-10-24 06:20:12 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:21:29 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:21:33 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:21:33 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:21:33 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:21:33 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:21:33 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:21:34 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:21:35 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:21:35 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:21:35 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:21:36 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:21:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo I 2025-10-24 06:21:36 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:22:58 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:23:00 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:23:01 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:23:01 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:23:03 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:23:03 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:23:03 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:23:04 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:23:04 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo I 2025-10-24 06:23:04 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:27:18 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:27:20 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:27:20 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:27:21 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:27:21 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:27:24 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:27:24 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:27:25 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:27:25 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:27:30 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
[NeMo W 2025-10-24 06:28:41 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:28:43 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:28:43 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:28:44 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:28:44 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:28:44 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:28:45 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:28:47 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:28:47 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:28:47 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:28:48 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:28:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo I 2025-10-24 06:28:49 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:30:44 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:30:47 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:30:47 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:30:48 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:30:51 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:30:51 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:30:51 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:30:52 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:30:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo I 2025-10-24 06:30:53 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo W 2025-10-24 06:32:33 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
    
[NeMo I 2025-10-24 06:32:35 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
[NeMo I 2025-10-24 06:32:35 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default
[NeMo W 2025-10-24 06:32:36 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has data parallel group : [5, 7]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has combined group of data parallel and context parallel : [5, 7]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 2], [1, 3], [4, 6], [5, 7]]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Ranks 5 has data parallel rank: 0
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has context parallel group: [5]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Ranks 5 has context parallel rank: 0
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has model parallel group: [0, 1, 4, 5]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] All model parallel group ranks: [[0, 1, 4, 5], [2, 3, 6, 7]]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has tensor model parallel group: [4, 5]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] All tensor model parallel group ranks: [[0, 1], [2, 3], [4, 5], [6, 7]]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has tensor model parallel rank: 1
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has pipeline model parallel group: [1, 5]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has embedding group: [1, 5]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has pipeline model parallel rank 1
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]
[NeMo I 2025-10-24 06:32:36 nemo_logging:393] Rank 5 has embedding rank: 1
[NeMo W 2025-10-24 06:32:38 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
[NeMo I 2025-10-24 06:32:38 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:32:38 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
[NeMo I 2025-10-24 06:32:39 nemo_logging:393] Padded vocab_size: 50432, original vocab_size: 50257, dummy tokens: 175.
[NeMo I 2025-10-24 06:32:40 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
[NeMo I 2025-10-24 06:32:40 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (1 ,1): 1848250368
