
=== Parallel Configuration Summary ===
DP:                        1 
TP:                        8
SP:                        Enabled
PP:                        1
VP:                        None
CP:                        1
Activation Recomputation:  Disabled
Micro Batch Size:          10
Global Batch Size:         10
======================================

──────── Entering Experiment mistral_7b_pretraining with id: mistral_7b_pretraining_1761281271 ─────────
[04:47:51] INFO     Log directory is:                                             local_scheduler.py:777
                    /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b                       
                    _pretraining_1761281271/mistral_7b_pretraining                                      
[04:47:51] Launching job mistral_7b_pretraining for experiment mistral_7b_pretraining  experiment.py:771
           INFO     Log directory is:                                             local_scheduler.py:777
                    /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b                       
                    _pretraining_1761281271/mistral_7b_pretraining                                      
           INFO     Launched app:                                                        launcher.py:111
                    local_persistent://nemo_run/mistral_7b_pretraining-g71ttmxphd1l1c                   
────────────────── Waiting for Experiment mistral_7b_pretraining_1761281271 to finish ──────────────────

Experiment Status for mistral_7b_pretraining_1761281271

Task 0: mistral_7b_pretraining
- Status: RUNNING
- Executor: LocalExecutor
- Job id: mistral_7b_pretraining-g71ttmxphd1l1c
- Local Directory: /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b_pretraining_1761281271/mistral_7b_pretraining

[04:47:52] INFO     Waiting for job mistral_7b_pretraining-g71ttmxphd1l1c to finish      launcher.py:131
                    [log=True]...                                                                       
retraining/0 I1024 04:47:53.537000 48451 torch/distributed/run.py:649] Using nproc_per_node=8.
retraining/0 W1024 04:47:53.538000 48451 torch/distributed/run.py:766] 
retraining/0 W1024 04:47:53.538000 48451 torch/distributed/run.py:766] *****************************************
retraining/0 W1024 04:47:53.538000 48451 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
retraining/0 W1024 04:47:53.538000 48451 torch/distributed/run.py:766] *****************************************
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195] Starting elastic_operator with launch configs:
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   entrypoint       : nemo_run.core.runners.fdl_runner
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   min_nodes        : 1
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   max_nodes        : 1
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   nproc_per_node   : 8
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   run_id           : 9997
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   rdzv_backend     : c10d
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   rdzv_endpoint    : localhost:0
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   rdzv_configs     : {'timeout': 900}
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   max_restarts     : 0
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   monitor_interval : 0.1
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   log_dir          : /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b_pretraining_1761281271/mistral_7b_pretraining/nemo_run/mistral_7b_pretraining-g71ttmxphd1l1c/torchelastic/mistral_7b_pretraining
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195]   metrics_cfg      : {}
retraining/0 I1024 04:47:53.539000 48451 torch/distributed/launcher/api.py:195] 
retraining/0 I1024 04:47:53.543000 48451 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python
retraining/0 I1024 04:47:53.543000 48451 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   master_addr=cc76e080f006
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   master_port=36223
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]
retraining/0 I1024 04:47:53.700000 48451 torch/distributed/elastic/agent/server/api.py:525] 
retraining/0 I1024 04:47:53.701000 48451 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group
retraining/0 I1024 04:47:53.702000 48451 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True
retraining/0 I1024 04:47:53.703000 48451 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
retraining/0 I1024 04:47:53.703000 48451 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.
retraining/0 [default0]:[NeMo W 2025-10-24 04:48:10 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
retraining/0 [default0]:      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
retraining/0 [default0]:    
retraining/0 [default6]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default6]:[W1024 04:48:12.340188268 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default6]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default6]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default1]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default1]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default1]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default1]:[W1024 04:48:13.158387443 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:13 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
retraining/0 [default5]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default5]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default4]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default4]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default4]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default5]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default4]:[W1024 04:48:13.900589419 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default5]:[W1024 04:48:13.926427555 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:GPU available: True (cuda), used: True
retraining/0 [default0]:TPU available: False, using: 0 TPU cores
retraining/0 [default0]:HPU available: False, using: 0 HPUs
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default/2025-10-24_04-48-14
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has data parallel group : [0]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Ranks 0 has data parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has context parallel group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Ranks 0 has context parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has embedding group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:14 nemo_logging:393] Rank 0 has embedding rank: 0
retraining/0 [default0]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default0]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default0]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default0]:[NeMo W 2025-10-24 04:48:14 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
retraining/0 [default0]:You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
retraining/0 [default0]:[W1024 04:48:14.165550055 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:----------------------------------------------------------------------------------------------------
retraining/0 [default0]:distributed_backend=nccl
retraining/0 [default0]:All distributed processes registered. Starting with 8 processes
retraining/0 [default0]:----------------------------------------------------------------------------------------------------
retraining/0 [default0]:
retraining/0 [default2]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default2]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default2]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default2]:[W1024 04:48:14.886461110 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default3]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default3]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default3]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default3]:[W1024 04:48:15.271396554 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default7]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default7]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default7]:[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
retraining/0 [default7]:[W1024 04:48:15.982650736 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] Let split_matrix = [(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)]
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] Building GPTDataset splits with sizes=[200, 0, 10] and config=GPTDatasetConfig(random_seed=1234, sequence_length=8192, blend=[['/root/dataset/openwebtext'], None], blend_per_split=None, split='900,50,50', split_matrix=[(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x7e4dcce4f830>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] Load the _IndexReader from /root/dataset/openwebtext.idx
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] 	Extract the sequence lengths
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] 	Extract the sequence pointers
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] 	Extract the document indices
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] > total number of sequences: 8013769
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] > total number of documents: 8013769
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:16 utils:661] Build and save the GPTDataset train indices
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] > total number of samples: 974208
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] > total number of epochs: 1
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] Load the GPTDataset valid indices
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] 	Load the document index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-document_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] 	Load the sample index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-sample_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] 	Load the shuffle index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-shuffle_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] > total number of samples: 53956
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] Build and save the GPTDataset test indices
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] > total number of samples: 54137
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] > total number of epochs: 1
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Padded vocab_size: 51200, original vocab_size: 50257, dummy tokens: 943.
retraining/0 [default0]:[NeMo W 2025-10-24 04:48:17 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 num_microbatches_calculator:228] setting number of microbatches to constant 1
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 898895872
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, reuse_grad_buf_for_mxfp8_param_ag=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False, nccl_ub=False, fsdp_double_buffer=False)
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 1
retraining/0 [default0]:    Params for bucket 1 (898895872 elements, 898895872 padded size):
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.embedding.word_embeddings.weight
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.final_layernorm.weight
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_proj.weight
retraining/0 [default0]:[NeMo I 2025-10-24 04:48:17 utils:661] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=9e-05, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp8_recipe='delayed', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
retraining/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default3]:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default4]:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default6]:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default7]:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default5]:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default2]:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default0]:
retraining/0 [default0]:  | Name   | Type | Params | Mode 
retraining/0 [default0]:----------------------------------------
retraining/0 [default0]:0 | module | DDP  | 898 M  | train
retraining/0 [default0]:----------------------------------------
retraining/0 [default0]:898 M     Trainable params
retraining/0 [default0]:0         Non-trainable params
retraining/0 [default0]:898 M     Total params
retraining/0 [default0]:3,595.583 Total estimated model params size (MB)
retraining/0 [default0]:651       Modules in train mode
retraining/0 [default0]:0         Modules in eval mode
retraining/0 [default0]:[NeMo W 2025-10-24 04:48:22 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
retraining/0 [default0]:[NeMo W 2025-10-24 04:48:22 rerun_state_machine:239] RerunStateMachine initialized in mode RerunMode.DISABLED
retraining/0 [default6]:[rank6]: Traceback (most recent call last):
retraining/0 [default6]:[rank6]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default6]:[rank6]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default6]:[rank6]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default6]:[rank6]:     fdl_runner_app()
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default6]:[rank6]:     raise e
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default6]:[rank6]:     return get_command(self)(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default6]:[rank6]:     return self.main(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default6]:[rank6]:     return _main(
retraining/0 [default6]:[rank6]:            ^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default6]:[rank6]:     rv = self.invoke(ctx)
retraining/0 [default6]:[rank6]:          ^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default6]:[rank6]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default6]:[rank6]:     return __callback(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default6]:[rank6]:     return callback(**use_params)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default6]:[rank6]:     fdl_fn()
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default6]:[rank6]:     return train(
retraining/0 [default6]:[rank6]:            ^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default6]:[rank6]:     trainer.fit(model, data)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default6]:[rank6]:     call._call_and_handle_interrupt(
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default6]:[rank6]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default6]:[rank6]:     return function(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default6]:[rank6]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default6]:[rank6]:     results = self._run_stage()
retraining/0 [default6]:[rank6]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default6]:[rank6]:     self.fit_loop.run()
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default6]:[rank6]:     self.advance()
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default6]:[rank6]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default6]:[rank6]:     self.advance(data_fetcher)
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default6]:[rank6]:     super().advance(data_fetcher)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default6]:[rank6]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default6]:[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default6]:[rank6]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default6]:[rank6]:     call._call_lightning_module_hook(
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default6]:[rank6]:     output = fn(*args, **kwargs)
retraining/0 [default6]:[rank6]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default6]:[rank6]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default6]:[rank6]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default6]:[rank6]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default6]:[rank6]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default6]:[rank6]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default6]:[rank6]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default6]:[rank6]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default6]:[rank6]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default6]:[rank6]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default6]:[rank6]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default6]:[rank6]:     loss = closure()
retraining/0 [default6]:[rank6]:            ^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default6]:[rank6]:     closure_result = closure()
retraining/0 [default6]:[rank6]:                      ^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default6]:[rank6]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default6]:[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default6]:[rank6]:     return func(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default6]:[rank6]:     step_output = self._step_fn()
retraining/0 [default6]:[rank6]:                   ^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default6]:[rank6]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default6]:[rank6]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default6]:[rank6]:     output = fn(*args, **kwargs)
retraining/0 [default6]:[rank6]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default6]:[rank6]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default6]:[rank6]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default6]:[rank6]:     return self._step(
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default6]:[rank6]:     return self.forward(
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default6]:[rank6]:     microbatch_outputs = step()
retraining/0 [default6]:[rank6]:                          ^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default6]:[rank6]:     return self.forward_backward_func(
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default6]:[rank6]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default6]:[rank6]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default6]:[rank6]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default6]:[rank6]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default6]:[rank6]:     Variable._execution_engine.run_backward(
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default6]:[rank6]:     return user_fn(self, *args)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default6]:[rank6]:     return bwd(*args, **kwargs)
retraining/0 [default6]:[rank6]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 522, in backward
retraining/0 [default6]:[rank6]:     grad_input = grad_output.matmul(weight)
retraining/0 [default6]:[rank6]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default6]:[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 6 has a total capacity of 79.25 GiB of which 300.94 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 75.62 GiB is allocated by PyTorch, and 923.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default3]:[rank3]: Traceback (most recent call last):
retraining/0 [default3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default3]:[rank3]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default3]:[rank3]:     fdl_runner_app()
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default3]:[rank3]:     raise e
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default3]:[rank3]:     return get_command(self)(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default3]:[rank3]:     return self.main(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default3]:[rank3]:     return _main(
retraining/0 [default3]:[rank3]:            ^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default3]:[rank3]:     rv = self.invoke(ctx)
retraining/0 [default3]:[rank3]:          ^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default3]:[rank3]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default3]:[rank3]:     return __callback(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default3]:[rank3]:     return callback(**use_params)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default3]:[rank3]:     fdl_fn()
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default3]:[rank3]:     return train(
retraining/0 [default3]:[rank3]:            ^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default3]:[rank3]:     trainer.fit(model, data)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default3]:[rank3]:     call._call_and_handle_interrupt(
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default3]:[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default3]:[rank3]:     return function(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default3]:[rank3]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default3]:[rank3]:     results = self._run_stage()
retraining/0 [default3]:[rank3]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default3]:[rank3]:     self.fit_loop.run()
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default3]:[rank3]:     self.advance()
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default3]:[rank3]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default3]:[rank3]:     self.advance(data_fetcher)
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default3]:[rank3]:     super().advance(data_fetcher)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default3]:[rank3]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default3]:[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default3]:[rank3]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default3]:[rank3]:     call._call_lightning_module_hook(
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default3]:[rank3]:     output = fn(*args, **kwargs)
retraining/0 [default3]:[rank3]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default3]:[rank3]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default3]:[rank3]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default3]:[rank3]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default3]:[rank3]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default3]:[rank3]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default3]:[rank3]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default3]:[rank3]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default3]:[rank3]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default3]:[rank3]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default3]:[rank3]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default3]:[rank3]:     loss = closure()
retraining/0 [default3]:[rank3]:            ^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default3]:[rank3]:     closure_result = closure()
retraining/0 [default3]:[rank3]:                      ^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default3]:[rank3]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default3]:[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default3]:[rank3]:     return func(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default3]:[rank3]:     step_output = self._step_fn()
retraining/0 [default3]:[rank3]:                   ^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default3]:[rank3]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default3]:[rank3]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default3]:[rank3]:     output = fn(*args, **kwargs)
retraining/0 [default3]:[rank3]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default3]:[rank3]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default3]:[rank3]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default3]:[rank3]:     return self._step(
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default3]:[rank3]:     return self.forward(
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default3]:[rank3]:     microbatch_outputs = step()
retraining/0 [default3]:[rank3]:                          ^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default3]:[rank3]:     return self.forward_backward_func(
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default3]:[rank3]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default3]:[rank3]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default3]:[rank3]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default3]:[rank3]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default3]:[rank3]:     Variable._execution_engine.run_backward(
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default3]:[rank3]:     return user_fn(self, *args)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default3]:[rank3]:     return bwd(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 522, in backward
retraining/0 [default3]:[rank3]:     grad_input = grad_output.matmul(weight)
retraining/0 [default3]:[rank3]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 3 has a total capacity of 79.25 GiB of which 300.94 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 75.62 GiB is allocated by PyTorch, and 923.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default2]:[rank2]: Traceback (most recent call last):
retraining/0 [default2]:[rank2]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default2]:[rank2]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default2]:[rank2]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default2]:[rank2]:     fdl_runner_app()
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default2]:[rank2]:     raise e
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default2]:[rank2]:     return get_command(self)(*args, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default2]:[rank2]:     return self.main(*args, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default2]:[rank2]:     return _main(
retraining/0 [default2]:[rank2]:            ^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default2]:[rank2]:     rv = self.invoke(ctx)
retraining/0 [default2]:[rank2]:          ^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default2]:[rank2]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default2]:[rank2]:     return __callback(*args, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default2]:[rank2]:     return callback(**use_params)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default2]:[rank2]:     fdl_fn()
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default2]:[rank2]:     return train(
retraining/0 [default2]:[rank2]:            ^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default2]:[rank2]:     trainer.fit(model, data)
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default2]:[rank2]:     call._call_and_handle_interrupt(
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default2]:[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default2]:[rank2]:     return function(*args, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default2]:[rank2]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default2]:[rank2]:     results = self._run_stage()
retraining/0 [default2]:[rank2]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default2]:[rank2]:     self.fit_loop.run()
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default2]:[rank2]:     self.advance()
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default2]:[rank2]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default2]:[rank2]:     self.advance(data_fetcher)
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default2]:[rank2]:     super().advance(data_fetcher)
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default2]:[rank2]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default2]:[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default2]:[rank2]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default2]:[rank2]:     call._call_lightning_module_hook(
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default2]:[rank2]:     output = fn(*args, **kwargs)
retraining/0 [default2]:[rank2]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default2]:[rank2]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default2]:[rank2]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default2]:[rank2]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default2]:[rank2]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default2]:[rank2]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default2]:[rank2]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default2]:[rank2]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default2]:[rank2]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default2]:[rank2]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default2]:[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default2]:[rank2]:     loss = closure()
retraining/0 [default2]:[rank2]:            ^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default2]:[rank2]:     closure_result = closure()
retraining/0 [default2]:[rank2]:                      ^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default2]:[rank2]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default2]:[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default2]:[rank2]:     return func(*args, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default2]:[rank2]:     step_output = self._step_fn()
retraining/0 [default2]:[rank2]:                   ^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default2]:[rank2]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default2]:[rank2]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default2]:[rank2]:     output = fn(*args, **kwargs)
retraining/0 [default2]:[rank2]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default2]:[rank2]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default2]:[rank2]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default2]:[rank2]:     return self._step(
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default2]:[rank2]:     return self.forward(
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default2]:[rank2]:     microbatch_outputs = step()
retraining/0 [default2]:[rank2]:                          ^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default2]:[rank2]:     return self.forward_backward_func(
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default2]:[rank2]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default2]:[rank2]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default2]:[rank2]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default2]:[rank2]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default2]:[rank2]:     Variable._execution_engine.run_backward(
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default2]:[rank2]:     return user_fn(self, *args)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default2]:[rank2]:     return bwd(*args, **kwargs)
retraining/0 [default2]:[rank2]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 522, in backward
retraining/0 [default2]:[rank2]:     grad_input = grad_output.matmul(weight)
retraining/0 [default2]:[rank2]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default2]:[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 2 has a total capacity of 79.25 GiB of which 300.94 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 75.62 GiB is allocated by PyTorch, and 923.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default1]:[rank1]: Traceback (most recent call last):
retraining/0 [default1]:[rank1]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default1]:[rank1]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default1]:[rank1]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default1]:[rank1]:     fdl_runner_app()
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default1]:[rank1]:     raise e
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default1]:[rank1]:     return get_command(self)(*args, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default1]:[rank1]:     return self.main(*args, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default1]:[rank1]:     return _main(
retraining/0 [default1]:[rank1]:            ^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default1]:[rank1]:     rv = self.invoke(ctx)
retraining/0 [default1]:[rank1]:          ^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default1]:[rank1]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default1]:[rank1]:     return __callback(*args, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default1]:[rank1]:     return callback(**use_params)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default1]:[rank1]:     fdl_fn()
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default1]:[rank1]:     return train(
retraining/0 [default1]:[rank1]:            ^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default1]:[rank1]:     trainer.fit(model, data)
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default1]:[rank1]:     call._call_and_handle_interrupt(
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default1]:[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default1]:[rank1]:     return function(*args, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default1]:[rank1]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default1]:[rank1]:     results = self._run_stage()
retraining/0 [default1]:[rank1]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default1]:[rank1]:     self.fit_loop.run()
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default1]:[rank1]:     self.advance()
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default1]:[rank1]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default1]:[rank1]:     self.advance(data_fetcher)
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default1]:[rank1]:     super().advance(data_fetcher)
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default1]:[rank1]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default1]:[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default1]:[rank1]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default1]:[rank1]:     call._call_lightning_module_hook(
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default1]:[rank1]:     output = fn(*args, **kwargs)
retraining/0 [default1]:[rank1]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default1]:[rank1]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default1]:[rank1]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default1]:[rank1]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default1]:[rank1]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default1]:[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default1]:[rank1]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default1]:[rank1]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default1]:[rank1]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default1]:[rank1]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default1]:[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default1]:[rank1]:     loss = closure()
retraining/0 [default1]:[rank1]:            ^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default1]:[rank1]:     closure_result = closure()
retraining/0 [default1]:[rank1]:                      ^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default1]:[rank1]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default1]:[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default1]:[rank1]:     return func(*args, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default1]:[rank1]:     step_output = self._step_fn()
retraining/0 [default1]:[rank1]:                   ^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default1]:[rank1]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default1]:[rank1]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default1]:[rank1]:     output = fn(*args, **kwargs)
retraining/0 [default1]:[rank1]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default1]:[rank1]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default1]:[rank1]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default1]:[rank1]:     return self._step(
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default1]:[rank1]:     return self.forward(
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default1]:[rank1]:     microbatch_outputs = step()
retraining/0 [default1]:[rank1]:                          ^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default1]:[rank1]:     return self.forward_backward_func(
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default1]:[rank1]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default1]:[rank1]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default1]:[rank1]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default1]:[rank1]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default1]:[rank1]:     Variable._execution_engine.run_backward(
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default1]:[rank1]:     return user_fn(self, *args)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default1]:[rank1]:     return bwd(*args, **kwargs)
retraining/0 [default1]:[rank1]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 522, in backward
retraining/0 [default1]:[rank1]:     grad_input = grad_output.matmul(weight)
retraining/0 [default1]:[rank1]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default1]:[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 1 has a total capacity of 79.25 GiB of which 300.94 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 75.62 GiB is allocated by PyTorch, and 923.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default5]:[rank5]: Traceback (most recent call last):
retraining/0 [default5]:[rank5]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default5]:[rank5]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default5]:[rank5]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default5]:[rank5]:     fdl_runner_app()
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default5]:[rank5]:     raise e
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default5]:[rank5]:     return get_command(self)(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default5]:[rank5]:     return self.main(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default5]:[rank5]:     return _main(
retraining/0 [default5]:[rank5]:            ^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default5]:[rank5]:     rv = self.invoke(ctx)
retraining/0 [default5]:[rank5]:          ^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default5]:[rank5]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default5]:[rank5]:     return __callback(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default5]:[rank5]:     return callback(**use_params)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default5]:[rank5]:     fdl_fn()
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default5]:[rank5]:     return train(
retraining/0 [default5]:[rank5]:            ^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default5]:[rank5]:     trainer.fit(model, data)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default5]:[rank5]:     call._call_and_handle_interrupt(
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default5]:[rank5]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default5]:[rank5]:     return function(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default5]:[rank5]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default5]:[rank5]:     results = self._run_stage()
retraining/0 [default5]:[rank5]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default5]:[rank5]:     self.fit_loop.run()
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default5]:[rank5]:     self.advance()
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default5]:[rank5]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default5]:[rank5]:     self.advance(data_fetcher)
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default5]:[rank5]:     super().advance(data_fetcher)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default5]:[rank5]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default5]:[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default5]:[rank5]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default5]:[rank5]:     call._call_lightning_module_hook(
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default5]:[rank5]:     output = fn(*args, **kwargs)
retraining/0 [default5]:[rank5]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default5]:[rank5]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default5]:[rank5]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default5]:[rank5]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default5]:[rank5]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default5]:[rank5]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default5]:[rank5]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default5]:[rank5]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default5]:[rank5]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default5]:[rank5]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default5]:[rank5]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default5]:[rank5]:     loss = closure()
retraining/0 [default5]:[rank5]:            ^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default5]:[rank5]:     closure_result = closure()
retraining/0 [default5]:[rank5]:                      ^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default5]:[rank5]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default5]:[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default5]:[rank5]:     return func(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default5]:[rank5]:     step_output = self._step_fn()
retraining/0 [default5]:[rank5]:                   ^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default5]:[rank5]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default5]:[rank5]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default5]:[rank5]:     output = fn(*args, **kwargs)
retraining/0 [default5]:[rank5]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default5]:[rank5]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default5]:[rank5]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default5]:[rank5]:     return self._step(
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default5]:[rank5]:     return self.forward(
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default5]:[rank5]:     microbatch_outputs = step()
retraining/0 [default5]:[rank5]:                          ^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default5]:[rank5]:     return self.forward_backward_func(
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default5]:[rank5]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default5]:[rank5]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default5]:[rank5]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default5]:[rank5]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default5]:[rank5]:     Variable._execution_engine.run_backward(
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default5]:[rank5]:     return user_fn(self, *args)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default5]:[rank5]:     return bwd(*args, **kwargs)
retraining/0 [default5]:[rank5]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 522, in backward
retraining/0 [default5]:[rank5]:     grad_input = grad_output.matmul(weight)
retraining/0 [default5]:[rank5]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default5]:[rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 5 has a total capacity of 79.25 GiB of which 300.94 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 75.62 GiB is allocated by PyTorch, and 923.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default0]:[rank0]: Traceback (most recent call last):
retraining/0 [default0]:[rank0]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default0]:[rank0]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default0]:[rank0]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default0]:[rank0]:     fdl_runner_app()
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default0]:[rank0]:     raise e
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default0]:[rank0]:     return get_command(self)(*args, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default0]:[rank0]:     return self.main(*args, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default0]:[rank0]:     return _main(
retraining/0 [default0]:[rank0]:            ^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default0]:[rank0]:     rv = self.invoke(ctx)
retraining/0 [default0]:[rank0]:          ^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default0]:[rank0]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default0]:[rank0]:     return __callback(*args, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default0]:[rank0]:     return callback(**use_params)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default0]:[rank0]:     fdl_fn()
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default0]:[rank0]:     return train(
retraining/0 [default0]:[rank0]:            ^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default0]:[rank0]:     trainer.fit(model, data)
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default0]:[rank0]:     call._call_and_handle_interrupt(
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default0]:[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default0]:[rank0]:     return function(*args, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default0]:[rank0]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default0]:[rank0]:     results = self._run_stage()
retraining/0 [default0]:[rank0]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default0]:[rank0]:     self.fit_loop.run()
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default0]:[rank0]:     self.advance()
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default0]:[rank0]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default0]:[rank0]:     self.advance(data_fetcher)
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default0]:[rank0]:     super().advance(data_fetcher)
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default0]:[rank0]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default0]:[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default0]:[rank0]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default0]:[rank0]:     call._call_lightning_module_hook(
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default0]:[rank0]:     output = fn(*args, **kwargs)
retraining/0 [default0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default0]:[rank0]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default0]:[rank0]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default0]:[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default0]:[rank0]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default0]:[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default0]:[rank0]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default0]:[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default0]:[rank0]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default0]:[rank0]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default0]:[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default0]:[rank0]:     loss = closure()
retraining/0 [default0]:[rank0]:            ^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default0]:[rank0]:     closure_result = closure()
retraining/0 [default0]:[rank0]:                      ^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default0]:[rank0]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default0]:[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default0]:[rank0]:     return func(*args, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default0]:[rank0]:     step_output = self._step_fn()
retraining/0 [default0]:[rank0]:                   ^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default0]:[rank0]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default0]:[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default0]:[rank0]:     output = fn(*args, **kwargs)
retraining/0 [default0]:[rank0]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default0]:[rank0]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default0]:[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default0]:[rank0]:     return self._step(
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default0]:[rank0]:     return self.forward(
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default0]:[rank0]:     microbatch_outputs = step()
retraining/0 [default0]:[rank0]:                          ^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default0]:[rank0]:     return self.forward_backward_func(
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default0]:[rank0]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default0]:[rank0]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default0]:[rank0]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default0]:[rank0]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default0]:[rank0]:     Variable._execution_engine.run_backward(
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default0]:[rank0]:     return user_fn(self, *args)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default0]:[rank0]:     return bwd(*args, **kwargs)
retraining/0 [default0]:[rank0]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 522, in backward
retraining/0 [default0]:[rank0]:     grad_input = grad_output.matmul(weight)
retraining/0 [default0]:[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default0]:[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 300.94 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 75.62 GiB is allocated by PyTorch, and 923.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default7]:[rank7]: Traceback (most recent call last):
retraining/0 [default7]:[rank7]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default7]:[rank7]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default7]:[rank7]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default7]:[rank7]:     fdl_runner_app()
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default7]:[rank7]:     raise e
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default7]:[rank7]:     return get_command(self)(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default7]:[rank7]:     return self.main(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default7]:[rank7]:     return _main(
retraining/0 [default7]:[rank7]:            ^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default7]:[rank7]:     rv = self.invoke(ctx)
retraining/0 [default7]:[rank7]:          ^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default7]:[rank7]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default7]:[rank7]:     return __callback(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default7]:[rank7]:     return callback(**use_params)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default7]:[rank7]:     fdl_fn()
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default7]:[rank7]:     return train(
retraining/0 [default7]:[rank7]:            ^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default7]:[rank7]:     trainer.fit(model, data)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default7]:[rank7]:     call._call_and_handle_interrupt(
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default7]:[rank7]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default7]:[rank7]:     return function(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default7]:[rank7]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default7]:[rank7]:     results = self._run_stage()
retraining/0 [default7]:[rank7]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default7]:[rank7]:     self.fit_loop.run()
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default7]:[rank7]:     self.advance()
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default7]:[rank7]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default7]:[rank7]:     self.advance(data_fetcher)
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default7]:[rank7]:     super().advance(data_fetcher)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default7]:[rank7]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default7]:[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default7]:[rank7]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default7]:[rank7]:     call._call_lightning_module_hook(
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default7]:[rank7]:     output = fn(*args, **kwargs)
retraining/0 [default7]:[rank7]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default7]:[rank7]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default7]:[rank7]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default7]:[rank7]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default7]:[rank7]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default7]:[rank7]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default7]:[rank7]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default7]:[rank7]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default7]:[rank7]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default7]:[rank7]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default7]:[rank7]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default7]:[rank7]:     loss = closure()
retraining/0 [default7]:[rank7]:            ^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default7]:[rank7]:     closure_result = closure()
retraining/0 [default7]:[rank7]:                      ^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default7]:[rank7]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default7]:[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default7]:[rank7]:     return func(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default7]:[rank7]:     step_output = self._step_fn()
retraining/0 [default7]:[rank7]:                   ^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default7]:[rank7]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default7]:[rank7]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default7]:[rank7]:     output = fn(*args, **kwargs)
retraining/0 [default7]:[rank7]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default7]:[rank7]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default7]:[rank7]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default7]:[rank7]:     return self._step(
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default7]:[rank7]:     return self.forward(
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default7]:[rank7]:     microbatch_outputs = step()
retraining/0 [default7]:[rank7]:                          ^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default7]:[rank7]:     return self.forward_backward_func(
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default7]:[rank7]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default7]:[rank7]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default7]:[rank7]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default7]:[rank7]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default7]:[rank7]:     Variable._execution_engine.run_backward(
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default7]:[rank7]:     return user_fn(self, *args)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default7]:[rank7]:     return bwd(*args, **kwargs)
retraining/0 [default7]:[rank7]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 522, in backward
retraining/0 [default7]:[rank7]:     grad_input = grad_output.matmul(weight)
retraining/0 [default7]:[rank7]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default7]:[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 7 has a total capacity of 79.25 GiB of which 300.94 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 75.62 GiB is allocated by PyTorch, and 923.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 [default4]:[rank4]: Traceback (most recent call last):
retraining/0 [default4]:[rank4]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default4]:[rank4]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default4]:[rank4]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default4]:[rank4]:     fdl_runner_app()
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default4]:[rank4]:     raise e
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default4]:[rank4]:     return get_command(self)(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default4]:[rank4]:     return self.main(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default4]:[rank4]:     return _main(
retraining/0 [default4]:[rank4]:            ^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default4]:[rank4]:     rv = self.invoke(ctx)
retraining/0 [default4]:[rank4]:          ^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default4]:[rank4]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default4]:[rank4]:     return __callback(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default4]:[rank4]:     return callback(**use_params)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default4]:[rank4]:     fdl_fn()
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default4]:[rank4]:     return train(
retraining/0 [default4]:[rank4]:            ^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default4]:[rank4]:     trainer.fit(model, data)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default4]:[rank4]:     call._call_and_handle_interrupt(
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default4]:[rank4]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default4]:[rank4]:     return function(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default4]:[rank4]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default4]:[rank4]:     results = self._run_stage()
retraining/0 [default4]:[rank4]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default4]:[rank4]:     self.fit_loop.run()
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default4]:[rank4]:     self.advance()
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default4]:[rank4]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default4]:[rank4]:     self.advance(data_fetcher)
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default4]:[rank4]:     super().advance(data_fetcher)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default4]:[rank4]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default4]:[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default4]:[rank4]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default4]:[rank4]:     call._call_lightning_module_hook(
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default4]:[rank4]:     output = fn(*args, **kwargs)
retraining/0 [default4]:[rank4]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default4]:[rank4]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default4]:[rank4]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default4]:[rank4]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default4]:[rank4]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default4]:[rank4]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default4]:[rank4]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default4]:[rank4]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default4]:[rank4]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default4]:[rank4]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default4]:[rank4]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default4]:[rank4]:     loss = closure()
retraining/0 [default4]:[rank4]:            ^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default4]:[rank4]:     closure_result = closure()
retraining/0 [default4]:[rank4]:                      ^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default4]:[rank4]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default4]:[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default4]:[rank4]:     return func(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default4]:[rank4]:     step_output = self._step_fn()
retraining/0 [default4]:[rank4]:                   ^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default4]:[rank4]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default4]:[rank4]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default4]:[rank4]:     output = fn(*args, **kwargs)
retraining/0 [default4]:[rank4]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default4]:[rank4]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default4]:[rank4]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default4]:[rank4]:     return self._step(
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default4]:[rank4]:     return self.forward(
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default4]:[rank4]:     microbatch_outputs = step()
retraining/0 [default4]:[rank4]:                          ^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default4]:[rank4]:     return self.forward_backward_func(
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default4]:[rank4]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default4]:[rank4]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default4]:[rank4]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default4]:[rank4]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default4]:[rank4]:     Variable._execution_engine.run_backward(
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default4]:[rank4]:     return user_fn(self, *args)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default4]:[rank4]:     return bwd(*args, **kwargs)
retraining/0 [default4]:[rank4]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 522, in backward
retraining/0 [default4]:[rank4]:     grad_input = grad_output.matmul(weight)
retraining/0 [default4]:[rank4]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default4]:[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 4 has a total capacity of 79.25 GiB of which 300.94 MiB is free. Including non-PyTorch memory, this process has 78.95 GiB memory in use. Of the allocated memory 75.62 GiB is allocated by PyTorch, and 923.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
retraining/0 W1024 04:48:59.664000 48451 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 48522 closing signal SIGTERM
retraining/0 W1024 04:48:59.666000 48451 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 48523 closing signal SIGTERM
retraining/0 W1024 04:48:59.666000 48451 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 48524 closing signal SIGTERM
retraining/0 W1024 04:48:59.667000 48451 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 48525 closing signal SIGTERM
retraining/0 W1024 04:48:59.668000 48451 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 48527 closing signal SIGTERM
retraining/0 W1024 04:48:59.668000 48451 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 48528 closing signal SIGTERM
retraining/0 W1024 04:48:59.669000 48451 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 48529 closing signal SIGTERM
retraining/0 E1024 04:49:01.516000 48451 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 4 (pid: 48526) of binary: /usr/bin/python
retraining/0 I1024 04:49:01.527000 48451 torch/distributed/elastic/multiprocessing/errors/__init__.py:368] ('local_rank %s FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html', 4)
retraining/0 Traceback (most recent call last):
retraining/0   File "/usr/local/bin/torchrun", line 33, in <module>
retraining/0     sys.exit(load_entry_point('torch==2.8.0a0+5228986c39.nv25.5', 'console_scripts', 'torchrun')())
retraining/0              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
retraining/0     return f(*args, **kwargs)
retraining/0            ^^^^^^^^^^^^^^^^^^
retraining/0   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in main
retraining/0     run(args)
retraining/0   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 883, in run
retraining/0     elastic_launch(
retraining/0   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 139, in __call__
retraining/0     return launch_agent(self._config, self._entrypoint, list(args))
retraining/0            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
retraining/0     raise ChildFailedError(
retraining/0 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
retraining/0 ============================================================
retraining/0 nemo_run.core.runners.fdl_runner FAILED
retraining/0 ------------------------------------------------------------
retraining/0 Failures:
retraining/0   <NO_OTHER_FAILURES>
retraining/0 ------------------------------------------------------------
retraining/0 Root Cause (first observed failure):
retraining/0 [0]:
retraining/0   time      : 2025-10-24_04:48:59
retraining/0   host      : cc76e080f006
retraining/0   rank      : 4 (local_rank: 4)
retraining/0   exitcode  : 1 (pid: 48526)
retraining/0   error_file: <N/A>
retraining/0   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
retraining/0 ============================================================
[04:49:02] INFO     Job mistral_7b_pretraining-g71ttmxphd1l1c finished: FAILED           launcher.py:161
                                                                                                        
# The experiment was run with the following tasks: ['mistral_7b_pretraining']                           
# You can inspect and reconstruct this experiment at a later point in time using:                       
experiment = run.Experiment.from_id("mistral_7b_pretraining_1761281271")                                
experiment.status() # Gets the overall status                                                           
experiment.logs("mistral_7b_pretraining") # Gets the log for the provided task                          
experiment.cancel("mistral_7b_pretraining") # Cancels the provided task if still running                
                                                                                                        
                                                                                                        
# You can inspect this experiment at a later point in time using the CLI as well:                       
nemo experiment status mistral_7b_pretraining_1761281271                                                
nemo experiment logs mistral_7b_pretraining_1761281271 0                                                
nemo experiment cancel mistral_7b_pretraining_1761281271 0                                              
                                                                                                        
