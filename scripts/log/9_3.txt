
=== Parallel Configuration Summary ===
DP:                        2 
TP:                        1
SP:                        Disabled
PP:                        1
VP:                        None
CP:                        4
Activation Recomputation:  Disabled
Micro Batch Size:          3
Global Batch Size:         6
======================================

──────── Entering Experiment mistral_7b_pretraining with id: mistral_7b_pretraining_1761282398 ─────────
[05:06:38] INFO     Log directory is:                                             local_scheduler.py:777
                    /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b                       
                    _pretraining_1761282398/mistral_7b_pretraining                                      
[05:06:38] Launching job mistral_7b_pretraining for experiment mistral_7b_pretraining  experiment.py:771
           INFO     Log directory is:                                             local_scheduler.py:777
                    /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b                       
                    _pretraining_1761282398/mistral_7b_pretraining                                      
           INFO     Launched app:                                                        launcher.py:111
                    local_persistent://nemo_run/mistral_7b_pretraining-w0ndtlmtq63z9                    
────────────────── Waiting for Experiment mistral_7b_pretraining_1761282398 to finish ──────────────────

Experiment Status for mistral_7b_pretraining_1761282398

Task 0: mistral_7b_pretraining
- Status: RUNNING
- Executor: LocalExecutor
- Job id: mistral_7b_pretraining-w0ndtlmtq63z9
- Local Directory: /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b_pretraining_1761282398/mistral_7b_pretraining

[05:06:39] INFO     Waiting for job mistral_7b_pretraining-w0ndtlmtq63z9 to finish       launcher.py:131
                    [log=True]...                                                                       
retraining/0 I1024 05:06:40.853000 62616 torch/distributed/run.py:649] Using nproc_per_node=8.
retraining/0 W1024 05:06:40.853000 62616 torch/distributed/run.py:766] 
retraining/0 W1024 05:06:40.853000 62616 torch/distributed/run.py:766] *****************************************
retraining/0 W1024 05:06:40.853000 62616 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
retraining/0 W1024 05:06:40.853000 62616 torch/distributed/run.py:766] *****************************************
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195] Starting elastic_operator with launch configs:
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   entrypoint       : nemo_run.core.runners.fdl_runner
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   min_nodes        : 1
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   max_nodes        : 1
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   nproc_per_node   : 8
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   run_id           : 5454
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   rdzv_backend     : c10d
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   rdzv_endpoint    : localhost:0
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   rdzv_configs     : {'timeout': 900}
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   max_restarts     : 0
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   monitor_interval : 0.1
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   log_dir          : /root/.nemo_run/experiments/mistral_7b_pretraining/mistral_7b_pretraining_1761282398/mistral_7b_pretraining/nemo_run/mistral_7b_pretraining-w0ndtlmtq63z9/torchelastic/mistral_7b_pretraining
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195]   metrics_cfg      : {}
retraining/0 I1024 05:06:40.854000 62616 torch/distributed/launcher/api.py:195] 
retraining/0 I1024 05:06:40.858000 62616 torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python
retraining/0 I1024 05:06:40.858000 62616 torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   master_addr=cc76e080f006
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   master_port=34409
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   group_rank=0
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=1
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]
retraining/0 I1024 05:06:41.030000 62616 torch/distributed/elastic/agent/server/api.py:525] 
retraining/0 I1024 05:06:41.031000 62616 torch/distributed/elastic/agent/server/api.py:685] [default] Starting worker group
retraining/0 I1024 05:06:41.031000 62616 torch/distributed/elastic/agent/server/local_elastic_agent.py:298] use_agent_store: True
retraining/0 I1024 05:06:41.032000 62616 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
retraining/0 I1024 05:06:41.033000 62616 torch/distributed/elastic/agent/server/local_elastic_agent.py:236] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.
retraining/0 [default0]:[NeMo W 2025-10-24 05:06:59 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
retraining/0 [default0]:      warn("Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", RuntimeWarning)
retraining/0 [default0]:    
retraining/0 [default2]:[W1024 05:07:00.203253139 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab,  merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False
retraining/0 [default1]:[W1024 05:07:02.168633986 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default5]:[W1024 05:07:02.171735582 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default4]:[W1024 05:07:02.698657412 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Experiments will be logged at /root/baseline/scripts/nemo_experiments/default/2025-10-24_05-07-02
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has data parallel group : [0, 4]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3, 4, 5, 6, 7]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3, 4, 5, 6, 7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Ranks 0 has data parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has context parallel group: [0, 1, 2, 3]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All context parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Ranks 0 has context parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has model parallel group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has tensor model parallel group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All tensor model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has tensor model parallel rank: 0
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has embedding group: [0]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has pipeline model parallel rank 0
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:02 nemo_logging:393] Rank 0 has embedding rank: 0
retraining/0 [default0]:GPU available: True (cuda), used: True
retraining/0 [default0]:TPU available: False, using: 0 TPU cores
retraining/0 [default0]:HPU available: False, using: 0 HPUs
retraining/0 [default0]:[NeMo W 2025-10-24 05:07:02 nemo_logging:405] "update_logger_directory" is True. Overwriting tensorboard logger "save_dir" to /root/baseline/scripts/nemo_experiments
retraining/0 [default0]:You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
retraining/0 [default0]:[W1024 05:07:02.023112791 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:----------------------------------------------------------------------------------------------------
retraining/0 [default0]:distributed_backend=nccl
retraining/0 [default0]:All distributed processes registered. Starting with 8 processes
retraining/0 [default0]:----------------------------------------------------------------------------------------------------
retraining/0 [default0]:
retraining/0 [default3]:[W1024 05:07:02.012809979 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default5]:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default5]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default5]:[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default1]:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default1]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default1]:[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default4]:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default4]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default4]:[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default2]:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default2]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default2]:[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default3]:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default3]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default3]:[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default0]:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default0]:[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default0]:[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default6]:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default6]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default6]:[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default7]:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default7]:[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
retraining/0 [default7]:[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
retraining/0 [default6]:[W1024 05:07:03.440514858 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default7]:[W1024 05:07:03.398524004 ProcessGroupNCCL.cpp:959] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] Let split_matrix = [(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] Building GPTDataset splits with sizes=[120, 0, 6] and config=GPTDatasetConfig(random_seed=1234, sequence_length=8192, blend=[['/root/dataset/openwebtext'], None], blend_per_split=None, split='900,50,50', split_matrix=[(0, 0.9), (0.9, 0.9500000000000001), (0.9500000000000001, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x78d089ea8350>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] Load the _IndexReader from /root/dataset/openwebtext.idx
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Extract the sequence lengths
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Extract the sequence pointers
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Extract the document indices
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] > total number of sequences: 8013769
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] > total number of documents: 8013769
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] Load the GPTDataset train indices
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the document index from ada79a2266a263841d84e0d4883d2210-GPTDataset-train-document_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the sample index from ada79a2266a263841d84e0d4883d2210-GPTDataset-train-sample_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the shuffle index from ada79a2266a263841d84e0d4883d2210-GPTDataset-train-shuffle_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] > total number of samples: 974208
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] Load the GPTDataset valid indices
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the document index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-document_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the sample index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-sample_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the shuffle index from 180546dee5d5a6d96e72125412370813-GPTDataset-valid-shuffle_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] > total number of samples: 53956
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] Load the GPTDataset test indices
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the document index from d6af9e40472650ed629c9eeb86a8df07-GPTDataset-test-document_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the sample index from d6af9e40472650ed629c9eeb86a8df07-GPTDataset-test-sample_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] 	Load the shuffle index from d6af9e40472650ed629c9eeb86a8df07-GPTDataset-test-shuffle_index.npy
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 utils:661] > total number of samples: 54137
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:04 nemo_logging:393] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.
retraining/0 [default0]:[NeMo W 2025-10-24 05:07:04 nemo_logging:405] Recommend using CUDA_DEVICE_MAX_CONNECTIONS=1 for best performance                         but get None
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:05 nemo_logging:393] Copying Trainer's 'max_steps' (20) to LR scheduler's 'max_steps'.
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:05 num_microbatches_calculator:228] setting number of microbatches to constant 1
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:05 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 7185633280
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:05 utils:661] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=134217728, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, reuse_grad_buf_for_mxfp8_param_ag=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=None, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False, nccl_ub=False, fsdp_double_buffer=False)
retraining/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default2]:LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default4]:LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default5]:LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default1]:LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default3]:LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default6]:LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default7]:LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:05 utils:682] Number of buckets for gradient all-reduce / reduce-scatter: 33
retraining/0 [default0]:    Params for bucket 1 (176164864 elements, 176164864 padded size):
retraining/0 [default0]:    	module.decoder.final_layernorm.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 2 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 3 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 4 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 5 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 6 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 7 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 8 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 9 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 10 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 11 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 12 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 13 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 14 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 15 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    Params for bucket 16 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.17.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc2.weight
retraining/0 [default0]:    Params for bucket 17 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.16.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 18 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 19 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 20 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 21 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 22 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 23 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 24 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 25 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 26 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 27 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 28 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.5.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 29 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 30 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc2.weight
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc1.weight
retraining/0 [default0]:    Params for bucket 31 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.2.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc2.weight
retraining/0 [default0]:    Params for bucket 32 (218112000 elements, 218112000 padded size):
retraining/0 [default0]:    	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc1.weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.decoder.layers.1.self_attention.linear_qkv.weight
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc2.weight
retraining/0 [default0]:    Params for bucket 33 (247996416 elements, 247996416 padded size):
retraining/0 [default0]:    	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_proj.weight
retraining/0 [default0]:    	module.embedding.word_embeddings.weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
retraining/0 [default0]:    	module.decoder.layers.0.self_attention.linear_qkv.weight
retraining/0 [default0]:[NeMo I 2025-10-24 05:07:05 utils:661] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=9e-05, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp8_recipe='delayed', fp16=False, bf16=True, reuse_grad_buf_for_mxfp8_param_ag=False, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
retraining/0 [default0]:
retraining/0 [default0]:  | Name   | Type | Params | Mode 
retraining/0 [default0]:----------------------------------------
retraining/0 [default0]:0 | module | DDP  | 7.2 B  | train
retraining/0 [default0]:----------------------------------------
retraining/0 [default0]:7.2 B     Trainable params
retraining/0 [default0]:0         Non-trainable params
retraining/0 [default0]:7.2 B     Total params
retraining/0 [default0]:28,742.533Total estimated model params size (MB)
retraining/0 [default0]:651       Modules in train mode
retraining/0 [default0]:0         Modules in eval mode
retraining/0 [default0]:[NeMo W 2025-10-24 05:07:15 rerun_state_machine:1263] Implicit initialization of Rerun State Machine!
retraining/0 [default0]:[NeMo W 2025-10-24 05:07:15 rerun_state_machine:239] RerunStateMachine initialized in mode RerunMode.DISABLED
retraining/0 [default3]:[rank3]: Traceback (most recent call last):
retraining/0 [default3]:[rank3]:   File "<frozen runpy>", line 198, in _run_module_as_main
retraining/0 [default3]:[rank3]:   File "<frozen runpy>", line 88, in _run_code
retraining/0 [default3]:[rank3]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 72, in <module>
retraining/0 [default3]:[rank3]:     fdl_runner_app()
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 340, in __call__
retraining/0 [default3]:[rank3]:     raise e
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 323, in __call__
retraining/0 [default3]:[rank3]:     return get_command(self)(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1161, in __call__
retraining/0 [default3]:[rank3]:     return self.main(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 677, in main
retraining/0 [default3]:[rank3]:     return _main(
retraining/0 [default3]:[rank3]:            ^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/core.py", line 195, in _main
retraining/0 [default3]:[rank3]:     rv = self.invoke(ctx)
retraining/0 [default3]:[rank3]:          ^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 1443, in invoke
retraining/0 [default3]:[rank3]:     return ctx.invoke(self.callback, **ctx.params)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/click/core.py", line 788, in invoke
retraining/0 [default3]:[rank3]:     return __callback(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/typer/main.py", line 698, in wrapper
retraining/0 [default3]:[rank3]:     return callback(**use_params)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/Run/nemo_run/core/runners/fdl_runner.py", line 68, in fdl_direct_run
retraining/0 [default3]:[rank3]:     fdl_fn()
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 181, in pretrain
retraining/0 [default3]:[rank3]:     return train(
retraining/0 [default3]:[rank3]:            ^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/collections/llm/api.py", line 136, in train
retraining/0 [default3]:[rank3]:     trainer.fit(model, data)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 538, in fit
retraining/0 [default3]:[rank3]:     call._call_and_handle_interrupt(
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
retraining/0 [default3]:[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
retraining/0 [default3]:[rank3]:     return function(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 574, in _fit_impl
retraining/0 [default3]:[rank3]:     self._run(model, ckpt_path=ckpt_path)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 981, in _run
retraining/0 [default3]:[rank3]:     results = self._run_stage()
retraining/0 [default3]:[rank3]:               ^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py", line 1025, in _run_stage
retraining/0 [default3]:[rank3]:     self.fit_loop.run()
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 205, in run
retraining/0 [default3]:[rank3]:     self.advance()
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py", line 363, in advance
retraining/0 [default3]:[rank3]:     self.epoch_loop.run(self._data_fetcher)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 140, in run
retraining/0 [default3]:[rank3]:     self.advance(data_fetcher)
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/pytorch/trainer.py", line 47, in advance
retraining/0 [default3]:[rank3]:     super().advance(data_fetcher)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 250, in advance
retraining/0 [default3]:[rank3]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
retraining/0 [default3]:[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 190, in run
retraining/0 [default3]:[rank3]:     self._optimizer_step(batch_idx, closure)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 268, in _optimizer_step
retraining/0 [default3]:[rank3]:     call._call_lightning_module_hook(
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 167, in _call_lightning_module_hook
retraining/0 [default3]:[rank3]:     output = fn(*args, **kwargs)
retraining/0 [default3]:[rank3]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py", line 1306, in optimizer_step
retraining/0 [default3]:[rank3]:     optimizer.step(closure=optimizer_closure)
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/optimizer.py", line 153, in step
retraining/0 [default3]:[rank3]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
retraining/0 [default3]:[rank3]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 792, in optimizer_step
retraining/0 [default3]:[rank3]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default3]:[rank3]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/ddp.py", line 270, in optimizer_step
retraining/0 [default3]:[rank3]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
retraining/0 [default3]:[rank3]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py", line 238, in optimizer_step
retraining/0 [default3]:[rank3]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 122, in optimizer_step
retraining/0 [default3]:[rank3]:     return optimizer.step(closure=closure, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py", line 124, in wrapper
retraining/0 [default3]:[rank3]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/core/optim/mcore_optim.py", line 129, in step
retraining/0 [default3]:[rank3]:     loss = closure()
retraining/0 [default3]:[rank3]:            ^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/plugins/precision/precision.py", line 108, in _wrap_closure
retraining/0 [default3]:[rank3]:     closure_result = closure()
retraining/0 [default3]:[rank3]:                      ^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 144, in __call__
retraining/0 [default3]:[rank3]:     self._result = self.closure(*args, **kwargs)
retraining/0 [default3]:[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 116, in decorate_context
retraining/0 [default3]:[rank3]:     return func(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 129, in closure
retraining/0 [default3]:[rank3]:     step_output = self._step_fn()
retraining/0 [default3]:[rank3]:                   ^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/optimization/automatic.py", line 317, in _training_step
retraining/0 [default3]:[rank3]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
retraining/0 [default3]:[rank3]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py", line 319, in _call_strategy_hook
retraining/0 [default3]:[rank3]:     output = fn(*args, **kwargs)
retraining/0 [default3]:[rank3]:              ^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/pytorch/strategies/megatron_strategy.py", line 724, in training_step
retraining/0 [default3]:[rank3]:     out = self.model.training_step(dataloader_iter, *args, **kwargs)
retraining/0 [default3]:[rank3]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 335, in training_step
retraining/0 [default3]:[rank3]:     return self._step(
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 447, in _step
retraining/0 [default3]:[rank3]:     return self.forward(
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 297, in forward
retraining/0 [default3]:[rank3]:     microbatch_outputs = step()
retraining/0 [default3]:[rank3]:                          ^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/NeMo/nemo/lightning/megatron_parallel.py", line 1225, in __call__
retraining/0 [default3]:[rank3]:     return self.forward_backward_func(
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 535, in forward_backward_no_pipelining
retraining/0 [default3]:[rank3]:     backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)
retraining/0 [default3]:[rank3]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 409, in backward_step
retraining/0 [default3]:[rank3]:     custom_backward(output_tensor[0], output_tensor_grad[0])
retraining/0 [default3]:[rank3]:   File "/opt/megatron-lm/megatron/core/pipeline_parallel/schedules.py", line 160, in custom_backward
retraining/0 [default3]:[rank3]:     Variable._execution_engine.run_backward(
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 307, in apply
retraining/0 [default3]:[rank3]:     return user_fn(self, *args)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py", line 563, in decorate_bwd
retraining/0 [default3]:[rank3]:     return bwd(*args, **kwargs)
retraining/0 [default3]:[rank3]:            ^^^^^^^^^^^^^^^^^^^^
retraining/0 [default3]:[rank3]:   File "/opt/megatron-lm/megatron/core/tensor_parallel/layers.py", line 571, in backward
retraining/0 [default3]:[rank3]:     grad_weight = torch.zeros(
retraining/0 [default3]:[rank3]:                   ^^^^^^^^^^^^
retraining/0 [default3]:[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 3 has a total capacity of 79.25 GiB of which 344.94 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 72.16 GiB is allocated by PyTorch, and 3.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
